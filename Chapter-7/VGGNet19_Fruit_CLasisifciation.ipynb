{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b332b0-ede1-47b7-8e0a-0daf6f116955",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "    #Copyright (c) 2023, 2024 , Prof. Radhamadhab Dalai, ITER , Siksha O Aanusandhan University\n",
    "    #Odisha, India,\n",
    "    #Author's email address :  radhamadhabdalai@soa.ac.in\n",
    " ########################################################################################################\n",
    "\n",
    "# Imports here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "traindir = '/content/drive/MyDrive/Naaniz/Indian Dishes'\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "  train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "  cost_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "  test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                          [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "  # TODO: Load the datasets with ImageFolder\n",
    "  train_data = datasets.ImageFolder(traindir, transform=train_transforms)\n",
    "  cost_data = datasets.ImageFolder(traindir, transform=cost_transforms)\n",
    "  test_data  = datasets.ImageFolder(traindir, transform=test_transforms)\n",
    "\n",
    "  num_train = len(train_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(valid_size * num_train))\n",
    "  np.random.shuffle(indices)\n",
    "  from torch.utils.data.sampler import SubsetRandomSampler\n",
    "  train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "  trainloader = torch.utils.data.DataLoader(train_data,sampler=train_sampler,batch_size=64)\n",
    "  valid_loader = torch.utils.data.DataLoader(cost_data,sampler=test_sampler,batch_size=32)\n",
    "  testloader = torch.utils.data.DataLoader(test_data,sampler=test_sampler,batch_size=32)\n",
    "  image_datasets = [train_data, cost_data, test_data]\n",
    "  dataloaders = [trainloader, valid_loader, testloader]\n",
    "  return image_datasets,dataloaders\n",
    " \n",
    "\n",
    "  \n",
    "image_datasets,dataloaders = load_split_train_test(traindir, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c49d16b-b04e-465f-8d4f-554c593251b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Naaniz/Naanizdish2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/My Drive/Naaniz/Naanizdish2.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     cat_to_name \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(cat_to_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Naaniz/Naanizdish2.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/content/drive/MyDrive/Naaniz/Naanizdish2.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "    \n",
    "print(cat_to_name)\n",
    "print(\"\\n Length:\", len(cat_to_name))\n",
    "     \n",
    "with open('/content/drive/MyDrive/Naaniz/Naanizdish2.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575312db-550f-4253-8149-94b0ced40653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Build and train your network\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load a pre-trained network\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mvgg19(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Build and train your network\n",
    "# Load a pre-trained network\n",
    "model = models.vgg19(pretrained=True)\n",
    "model\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout.# Defin \n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(25088, 1024)),\n",
    "                          ('drop', nn.Dropout(p=0.5)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(1024, 231)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier\n",
    "model\n",
    "\n",
    "saved_file = \"/content/drive/MyDrive/epoch/lessloss%.pth\"\n",
    "state_dict = torch.load(saved_file)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# Train the classifier layers using backpropagation using the pre-trained network to get the features.\n",
    "# Track the loss and accuracy on the validation set to determine the best hyperparameters.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Train the classifier layers using backpropagation using the pre-trained network to get the features.\n",
    "# Track the loss and accuracy on the validation set to determine the best hyperparameters.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "best=100\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "    \n",
    "running_loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "start = time.time()\n",
    "print('Training started')\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_mode = 0\n",
    "    valid_mode = 1\n",
    "    \n",
    "    for mode in [train_mode, valid_mode]:   \n",
    "        if mode == train_mode:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        pass_count = 0\n",
    "        \n",
    "        for data in dataloaders[mode]:\n",
    "            pass_count += 1\n",
    "            inputs, labels = data\n",
    "            if cuda == True:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward\n",
    "            output = model.forward(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backward\n",
    "            if mode == train_mode:\n",
    "                loss.backward()\n",
    "                optimizer.step()                \n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ps = torch.exp(output).data\n",
    "            equality = (labels.data == ps.max(1)[1])\n",
    "            accuracy = equality.type_as(torch.cuda.FloatTensor()).mean()\n",
    "        if mode == train_mode:\n",
    "            print(\"\\nEpoch: {}/{} \".format(e+1, epochs),\n",
    "                  \"\\nTraining Loss: {:.4f}  \".format(running_loss/pass_count))\n",
    "        else:\n",
    "            print(\"Validation Loss: {:.4f}  \".format(running_loss/pass_count),\n",
    "              \"Accuracy: {:.4f}\".format(accuracy))\n",
    "            if best > running_loss/pass_count:\n",
    "              torch.save(model.state_dict(), \"/content/drive/MyDrive/epoch/lessloss%.pth\")\n",
    "              best = running_loss/pass_count\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "time_elapsed = time.time() - start\n",
    "print(\"\\nTotal time: {:.0f}m {:.0f}s\".format(time_elapsed//60, time_elapsed % 60))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f670fd-0c55-43a8-9bf3-3413c5476528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "    \n",
    "pass_count = 0\n",
    "\n",
    "for data in dataloaders[2]:\n",
    "    pass_count += 1\n",
    "    images, labels = data\n",
    "    \n",
    "    if cuda == True:\n",
    "        images, labels = Variable(images.cuda()), Variable(labels.cuda())\n",
    "    else:\n",
    "        images, labels = Variable(images), Variable(labels)\n",
    "\n",
    "    output = model.forward(images)\n",
    "    ps = torch.exp(output).data\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy/pass_count))\n",
    "\n",
    "# TODO: Save the checkpoint \n",
    "model.class_to_idx = image_datasets[0].class_to_idx\n",
    "\n",
    "checkpoint = {'input_size': 25088,\n",
    "              'output_size': 231,\n",
    "              'arch': 'vgg19',\n",
    "              'learning_rate': 0.01,\n",
    "              'batch_size': 64,\n",
    "              'classifier' : classifier,\n",
    "              'epochs': epochs,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_to_idx': model.class_to_idx}\n",
    "\n",
    "torch.save(checkpoint, '/content/drive/MyDrive/epoch/checkpoint%.pth')\n",
    "\n",
    "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
    "def load_checkpoint(filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    learning_rate = checkpoint['learning_rate']\n",
    "    model = getattr(torchvision.models, checkpoint['arch'])(pretrained=True)\n",
    "    model.classifier = checkpoint['classifier']\n",
    "    model.epochs = checkpoint['epochs']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "    return model, optimizer\n",
    "\n",
    "nn_filename = '/content/drive/MyDrive/epoch/checkpoint%.pth'\n",
    "\n",
    "model, optimizer = load_checkpoint(nn_filename)\n",
    "\n",
    "chkp_model = print(model)\n",
    "\n",
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # TODO: Process a PIL image for use in a PyTorch model\n",
    "    im = Image.open(image)\n",
    "    im = im.resize((256,256))\n",
    "    value = 0.5*(256-224)\n",
    "    im = im.crop((value,value,256-value,256-value))\n",
    "    im = np.array(im)/255\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    im = (im - mean) / std\n",
    "\n",
    "    return im.transpose(2,0,1)\n",
    "\n",
    "\n",
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Show original picture\n",
    "\n",
    "img_path = '/content/drive/MyDrive/Naaniz/test/1.jpg' \n",
    "\n",
    "with Image.open(img_path) as image:\n",
    "    plt.imshow(image)\n",
    "\n",
    "def predict(image_path, model, topk=6):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    # move the model to cuda\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        # Move model parameters to the GPU\n",
    "        model.cuda()\n",
    "        print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "        print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.device_count()-1))\n",
    "    else:\n",
    "        model.cpu()\n",
    "        print(\"We go for CPU\")\n",
    "    \n",
    "    # turn off dropout\n",
    "    model.eval()\n",
    "\n",
    "    # The image\n",
    "    image = process_image(image_path)\n",
    "    \n",
    "    # tranfer to tensor\n",
    "    image = torch.from_numpy(np.array([image])).float()\n",
    "    \n",
    "    # The image becomes the input\n",
    "    image = Variable(image)\n",
    "    if cuda:\n",
    "        image = image.cuda()\n",
    "        \n",
    "    output = model.forward(image)\n",
    "    \n",
    "    probabilities = torch.exp(output).data\n",
    "    \n",
    "    # getting the topk (=5) probabilites and indexes\n",
    "    # 0 -> probabilities\n",
    "    # 1 -> index\n",
    "    prob = torch.topk(probabilities, topk)[0].tolist()[0] # probabilities\n",
    "    index = torch.topk(probabilities, topk)[1].tolist()[0] # index\n",
    "    \n",
    "    ind = []\n",
    "    for i in range(len(model.class_to_idx.items())):\n",
    "        ind.append(list(model.class_to_idx.items())[i][0])\n",
    "\n",
    "    # transfer index to label\n",
    "    label = []\n",
    "    for i in range(6):\n",
    "        label.append(ind[index[i]])\n",
    "\n",
    "    return prob, label\n",
    "\n",
    "#img = random.choice(os.listdir('/content/drive/My Drive/Car Brand/Datasets/valid/aud\n",
    "img_path = '/content/drive/My Drive/Naaniz/test3/1.jpg' \n",
    "\n",
    "with Image.open(img_path) as image:\n",
    "    plt.imshow(image)\n",
    "with  Image.open(img_path) as image:\n",
    "    plt.imshow(image)\n",
    "prob, classes = predict(img_path, model)\n",
    "print(prob)\n",
    "print(classes)\n",
    "print([cat_to_name[x] for x in classes])\n",
    "\n",
    "\n",
    "img_path = '/content/drive/My Drive/Naaniz/test/11.jpg' \n",
    "prob, classes = predict(img_path, model)\n",
    "max_index = np.argmax(prob)\n",
    "max_probability = prob[max_index]\n",
    "label = classes[max_index]\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1 = plt.subplot2grid((15,9), (0,0), colspan=9, rowspan=9)\n",
    "ax2 = plt.subplot2grid((15,9), (9,2), colspan=5, rowspan=5)\n",
    "image = Image.open(img_path)\n",
    "ax1.axis('off') \n",
    "ax1.set_title(label)  \n",
    "ax1.imshow(image)\n",
    "labels = []\n",
    "for cl in classes:\n",
    "    labels.append(cl)   \n",
    "y_pos = np.arange(5)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(labels)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.invert_yaxis()\n",
    "ax2.barh(y_pos, prob, xerr=0, align='center', color='blue')\n",
    "plt.show()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
