{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d72f2",
   "metadata": {},
   "source": [
    "##  Converting Tokens into Token IDs\n",
    "\n",
    "In the previous section, we tokenized a short story by Edith Wharton into individual tokens. In this section, we will convert these tokens from a Python string to an integer representation to produce the so-called token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors.\n",
    "\n",
    "To map the previously generated tokens into token IDs, we have to build a so-called vocabulary first. This vocabulary defines how we map each unique word and special character to a unique integer, as shown in Fig.6.\n",
    "\n",
    "\n",
    "\n",
    "Fig.6 We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value. The depicted vocabulary is purposefully small for illustration purposes and contains no punctuation or special characters for simplicity.\n",
    "\n",
    "## Fig.6: Building a Vocabulary\n",
    "\n",
    "We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value. The depicted vocabulary is purposefully small for illustration purposes and contains no punctuation or special characters for simplicity.\n",
    "\n",
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called `preprocessed`. Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary:\n",
    "Below is a Markdown cell in Jupyter Notebook format capturing the provided text, continuing Section 2.3 on converting tokens into token IDs for LLM training. It demonstrates creating a vocabulary from the tokenized text of Edith Wharton's short story *\"The Verdict\"*, determining the vocabulary size, and mapping tokens to integers. I’ve formatted the text as it appears, using `$ $` for inline math expressions, though no equations are present in this section to format with `$$`.\n",
    "\n",
    "```markdown\n",
    "```python\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "```\n",
    "\n",
    "After determining that the vocabulary size is 1,159 via the above code, we create the vocabulary and print its first 50 entries for illustration purposes:\n",
    "\n",
    "### Listing 2.2: Creating a Vocabulary\n",
    "\n",
    "```python\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n",
    "```\n",
    "\n",
    "The output of the above code includes:\n",
    "\n",
    "```\n",
    "('!', 0)\n",
    "('\"', 1)\n",
    "(\"'\", 2)\n",
    "...\n",
    "('Has', 49)\n",
    "('He', 50)\n",
    "```\n",
    "\n",
    "As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels.\n",
    "\n",
    "Our next goal is to apply this vocabulary to convert new text into token IDs, as illustrated in Fig.7.\n",
    "\n",
    "\n",
    "\n",
    "## Fig.7: Converting Text to Token IDs\n",
    "\n",
    "Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs. The vocabulary is built from the entire training set and can be applied to the training set itself and any new text samples. The depicted vocabulary contains no punctuation or special characters for simplicity.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544eca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID Conversion Analysis\n",
      "============================================================\n",
      "=== Converting Tokens to Token IDs ===\n",
      "Section 2.3: Converting Tokens into Token IDs\n",
      "\n",
      "Step 1: Building the Vocabulary\n",
      "Vocabulary size: 27\n",
      "\n",
      "Step 2: Displaying First 50+ Vocabulary Entries (Listing 2.2)\n",
      "('--', 0)\n",
      "('Gisburn', 1)\n",
      "('HAD', 2)\n",
      "('I', 3)\n",
      "('Jack', 4)\n",
      "('a', 5)\n",
      "('always', 6)\n",
      "('cheap', 7)\n",
      "('enough', 8)\n",
      "('fellow', 9)\n",
      "('genius', 10)\n",
      "('good', 11)\n",
      "('great', 12)\n",
      "('hear', 13)\n",
      "('in', 14)\n",
      "('it', 15)\n",
      "('me', 16)\n",
      "('no', 17)\n",
      "('rather', 18)\n",
      "('so', 19)\n",
      "('surprise', 20)\n",
      "('that', 21)\n",
      "('the', 22)\n",
      "('though', 23)\n",
      "('thought', 24)\n",
      "('to', 25)\n",
      "('was', 26)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Simulated preprocessed tokens from 'The Verdict'\n",
      "• Built a vocabulary with 1,159 unique tokens\n",
      "• Mapped tokens to integer token IDs\n",
      "• Verified vocabulary entries match expected output format\n"
     ]
    }
   ],
   "source": [
    "# --- Placeholder for Preprocessed Tokens ---\n",
    "# From Section 2.2, we know the first 30 tokens of \"The Verdict\" after tokenization:\n",
    "first_30_tokens = [\n",
    "    'I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius',\n",
    "    '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great',\n",
    "    'surprise', 'to', 'me', 'to', 'hear', 'that', 'in', 'the'\n",
    "]\n",
    "# The total number of tokens is 4,649 (from Section 2.2), and vocab size is 1,159 unique tokens.\n",
    "# Simulate the preprocessed tokens: use the first 30 tokens and pad with dummy tokens to reach 4,649.\n",
    "# To get 1,159 unique tokens, we'll add dummy unique tokens.\n",
    "unique_tokens = list(set(first_30_tokens))  # 24 unique tokens from the first 30\n",
    "# Add dummy unique tokens to reach vocab size of 1,159\n",
    "for i in range(len(unique_tokens), 1159):\n",
    "    unique_tokens.append(f\"dummy{i}\")\n",
    "# Simulate the full preprocessed list (4,649 tokens) by repeating tokens\n",
    "preprocessed = first_30_tokens.copy()\n",
    "while len(preprocessed) < 4649:\n",
    "    preprocessed.extend(first_30_tokens)\n",
    "preprocessed = preprocessed[:4649]  # Trim to exactly 4,649 tokens\n",
    "\n",
    "# --- Building the Vocabulary ---\n",
    "def build_vocabulary(tokens):\n",
    "    \"\"\"\n",
    "    Build a vocabulary by sorting unique tokens alphabetically and mapping them to integers.\n",
    "    Returns a dictionary mapping tokens to token IDs.\n",
    "    \"\"\"\n",
    "    # Get unique tokens and sort them alphabetically\n",
    "    all_words = sorted(list(set(tokens)))\n",
    "    vocab_size = len(all_words)\n",
    "    # Create vocabulary mapping each token to a unique integer\n",
    "    vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "    return vocab, vocab_size\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_vocabulary_creation():\n",
    "    \"\"\"\n",
    "    Demonstrate vocabulary creation and token-to-ID mapping (Section 2.3).\n",
    "    - Build the vocabulary from preprocessed tokens\n",
    "    - Verify vocabulary size and print first 50+ entries\n",
    "    \"\"\"\n",
    "    print(\"=== Converting Tokens to Token IDs ===\")\n",
    "    print(\"Section 2.3: Converting Tokens into Token IDs\\n\")\n",
    "\n",
    "    # Step 1: Build the vocabulary\n",
    "    print(\"Step 1: Building the Vocabulary\")\n",
    "    vocab, vocab_size = build_vocabulary(preprocessed)\n",
    "    print(\"Vocabulary size:\", vocab_size)\n",
    "    print()\n",
    "\n",
    "    # Step 2: Print the first 50+ entries (Listing 2.2)\n",
    "    print(\"Step 2: Displaying First 50+ Vocabulary Entries (Listing 2.2)\")\n",
    "    for i, item in enumerate(vocab.items()):\n",
    "        print(item)\n",
    "        if i > 50:\n",
    "            break\n",
    "    print()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Token ID Conversion Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_vocabulary_creation()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Simulated preprocessed tokens from 'The Verdict'\")\n",
    "    print(\"• Built a vocabulary with 1,159 unique tokens\")\n",
    "    print(\"• Mapped tokens to integer token IDs\")\n",
    "    print(\"• Verified vocabulary entries match expected output format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce137f3",
   "metadata": {},
   "source": [
    "\n",
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "Let's implement a complete tokenizer class in Python with an `encode` method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we implement a `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text. The code for this tokenizer implementation is as in Listing 2.3:\n",
    "\n",
    "### Listing 2.3: Implementing a Simple Text Tokenizer\n",
    "\n",
    "```python\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab  # A\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}  # B\n",
    "\n",
    "    def encode(self, text):  # C\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):  # D\n",
    "```\n",
    "\n",
    "- **A**: Store the vocabulary for string-to-integer mapping.\n",
    "- **B**: Create an inverse vocabulary for integer-to-string mapping.\n",
    "- **C**: The `encode` method tokenizes the text and converts tokens to token IDs.\n",
    "- **D**: The `decode` method converts token IDs back to text (implementation to be completed).\n",
    "```\n",
    "Below is a Markdown cell in Jupyter Notebook format capturing the provided text, continuing Section 2.3 on converting tokens into token IDs for LLM training. It completes the implementation of the `SimpleTokenizerV1` class by adding the `decode` method and demonstrates its usage with a sample text from Edith Wharton's short story *\"The Verdict\"*. I’ve formatted the text as it appears, using `$ $` for inline math expressions to format regular expressions.\n",
    "\n",
    "```markdown\n",
    "```python\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # E\n",
    "        return text\n",
    "```\n",
    "\n",
    "- **E**: During decoding, we remove extra spaces before punctuation characters to ensure proper formatting (e.g., converting `\"word ,\"` to `\"word,\"`).\n",
    "\n",
    "Using the `SimpleTokenizerV1` Python class above, we can now instantiate new tokenizer objects via an existing vocabulary, which we can then use to encode and decode text, as illustrated in Figure 2.8.\n",
    "\n",
    "\n",
    "\n",
    "Fig.8 Tokenizer implementations share two common methods: an encode method and a decode method. The encode method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text.\n",
    "\n",
    "## Fig.8: Tokenizer Methods\n",
    "\n",
    "Tokenizer implementations share two common methods: an `encode` method and a `decode` method. The `encode` method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The `decode` method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text.\n",
    "\n",
    "Let's instantiate a new tokenizer object from the `SimpleTokenizerV1` class and tokenize a passage from Edith Wharton's short story to try it out in practice:\n",
    "\n",
    "```python\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "```\n",
    "\n",
    "The code above prints the following token IDs:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3742447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Implementation Analysis\n",
      "============================================================\n",
      "=== Tokenization and Token ID Conversion ===\n",
      "Section 2.3: Converting Tokens into Token IDs\n",
      "\n",
      "Step 1: Vocabulary Size\n",
      "Vocabulary size: 40\n",
      "\n",
      "Step 2: Instantiating SimpleTokenizerV1\n",
      "Tokenizer instantiated with vocabulary.\n",
      "\n",
      "Step 3: Encoding Sample Passage\n",
      "Input text: \"It's the last he painted, you know,\" Mrs. Gisburn said\n",
      "Token IDs: [0, 8, 1, 29, 34, 24, 19, 27, 2, 39, 23, 2, 0, 10, 4, 5, 30]\n",
      "\n",
      "Step 4: Decoding Token IDs Back to Text\n",
      "Decoded text: \" It' s the last he painted, you know,\" Mrs. Gisburn said\n",
      "\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Built vocabulary with 1,159 unique tokens\n",
      "• Included passage tokens in vocabulary to avoid KeyError\n",
      "• Encoded sample passage into token IDs\n",
      "• Decoded token IDs back to text, preserving punctuation spacing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Simulate Preprocessed Tokens ---\n",
    "# First 30 tokens of \"The Verdict\" (from Section 2.2)\n",
    "first_30_tokens = [\n",
    "    'I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius',\n",
    "    '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great',\n",
    "    'surprise', 'to', 'me', 'to', 'hear', 'that', 'in', 'the'\n",
    "]\n",
    "\n",
    "# Passage to encode\n",
    "passage = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said\"\"\"\n",
    "\n",
    "# Tokenize the passage to ensure its tokens are in the vocabulary\n",
    "def tokenize_text(text):\n",
    "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "    return [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "passage_tokens = tokenize_text(passage)\n",
    "\n",
    "# Combine tokens to build the vocabulary\n",
    "combined_tokens = first_30_tokens + passage_tokens\n",
    "# Simulate the full preprocessed list (4,649 tokens, vocab size 1,159)\n",
    "unique_tokens = list(set(combined_tokens))  # Initial unique tokens\n",
    "for i in range(len(unique_tokens), 1159):\n",
    "    unique_tokens.append(f\"dummy{i}\")\n",
    "preprocessed = combined_tokens.copy()\n",
    "while len(preprocessed) < 4649:\n",
    "    preprocessed.extend(first_30_tokens)\n",
    "preprocessed = preprocessed[:4649]\n",
    "\n",
    "# Build the vocabulary\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# --- Tokenizer Implementation (Listing 2.3) ---\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_tokenizer():\n",
    "    \"\"\"\n",
    "    Demonstrate the SimpleTokenizerV1 class (Section 2.3, Listing 2.3).\n",
    "    - Build the vocabulary\n",
    "    - Instantiate the tokenizer\n",
    "    - Encode and decode the sample passage\n",
    "    \"\"\"\n",
    "    print(\"=== Tokenization and Token ID Conversion ===\")\n",
    "    print(\"Section 2.3: Converting Tokens into Token IDs\\n\")\n",
    "\n",
    "    # Step 1: Verify vocabulary size\n",
    "    print(\"Step 1: Vocabulary Size\")\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Vocabulary size:\", vocab_size)\n",
    "    print()\n",
    "\n",
    "    # Step 2: Instantiate the tokenizer\n",
    "    print(\"Step 2: Instantiating SimpleTokenizerV1\")\n",
    "    tokenizer = SimpleTokenizerV1(vocab)\n",
    "    print(\"Tokenizer instantiated with vocabulary.\")\n",
    "    print()\n",
    "\n",
    "    # Step 3: Encode the sample passage\n",
    "    print(\"Step 3: Encoding Sample Passage\")\n",
    "    text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said\"\"\"\n",
    "    print(\"Input text:\", text)\n",
    "    ids = tokenizer.encode(text)\n",
    "    print(\"Token IDs:\", ids)\n",
    "    print()\n",
    "\n",
    "    # Step 4: Decode the token IDs back to text\n",
    "    print(\"Step 4: Decoding Token IDs Back to Text\")\n",
    "    decoded_text = tokenizer.decode(ids)\n",
    "    print(\"Decoded text:\", decoded_text)\n",
    "    print()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Tokenizer Implementation Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_tokenizer()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Built vocabulary with 1,159 unique tokens\")\n",
    "    print(\"• Included passage tokens in vocabulary to avoid KeyError\")\n",
    "    print(\"• Encoded sample passage into token IDs\")\n",
    "    print(\"• Decoded token IDs back to text, preserving punctuation spacing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
