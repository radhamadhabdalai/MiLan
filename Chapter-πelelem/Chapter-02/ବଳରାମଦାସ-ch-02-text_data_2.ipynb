{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339b534",
   "metadata": {},
   "source": [
    "the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the `[PAD]` token, up to the length of the longest text in the batch.\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token for simplicity. The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above. Also, `<|endoftext|>` is used for padding as well. However, as we'll explore in subsequent chapters when training on batched inputs, we typically use a mask, meaning we don't attend to padded tokens. Thus, the specific token chosen for padding becomes inconsequential.\n",
    "\n",
    "Moreover, the tokenizer used for GPT models also doesn't use an `<|unk|>` token for out-of-vocabulary words. Instead, GPT models use a **byte pair encoding** tokenizer, which breaks down words into subword units, which we will discuss in the next section.\n",
    "\n",
    "## 2.5 Byte pair encoding\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration purposes. This section covers a more sophisticated tokenization scheme based on a concept called **byte pair encoding (BPE)**. The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python open-source library called `tiktoken` (https://github.com/openai/tiktoken), which implements the BPE algorithm very efficiently based on source code in Rust. Similar to other Python libraries, we can install the `tiktoken` library via Python's pip installer from the terminal:\n",
    "\n",
    "```bash\n",
    "pip install tiktoken\n",
    "\n",
    "The code in this chapter is based on tiktoken 0.5.1. You can use the following code to check the version you currently have installed:\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method:\n",
    "\n",
    "Python\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit ter\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "The code above prints the following token IDs:\n",
    "\n",
    "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252]\n",
    "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2 earlier:\n",
    "\n",
    "Python\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n",
    "The above code prints the following:\n",
    "\n",
    "'Hello, do you like tea? <|endoftext|> In the sunlit terraces o'\n",
    "We can make two noteworthy observations based on the token IDs and decoded text above. First, the <|endoftext|> token is assigned a relatively large token ID, namely, 50256. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID. Second, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly. The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens? The algorithm underlying BPE breaks down words that aren't in its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fdaded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Installation of tiktoken ---\n",
      "To install the tiktoken library, please run the following command in your terminal:\n",
      "```bash\n",
      "pip install tiktoken\n",
      "```\n",
      "\n",
      "Please ensure you have pip installed and an active internet connection.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Checking tiktoken Version ---\n",
      "tiktoken version: 0.5.1 (Simulated output)\n",
      "Note: In a real environment, you would run the 'from importlib.metadata import version' code to get the actual version.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Demonstrating tiktoken Usage ---\n",
      "\n",
      "Error: 'tiktoken' library not found. Please run 'pip install tiktoken' first.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Installation Command ---\n",
    "print(\"--- Installation of tiktoken ---\")\n",
    "print(\"To install the tiktoken library, please run the following command in your terminal:\")\n",
    "print(\"```bash\")\n",
    "print(\"pip install tiktoken\")\n",
    "print(\"```\")\n",
    "print(\"\\nPlease ensure you have pip installed and an active internet connection.\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- 2. Simulate Version Check ---\n",
    "print(\"\\n--- Checking tiktoken Version ---\")\n",
    "# In a real environment, you would run this:\n",
    "# from importlib.metadata import version\n",
    "# import tiktoken\n",
    "# print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "# Simulating the output as per the text's example (0.5.1)\n",
    "print(\"tiktoken version: 0.5.1 (Simulated output)\")\n",
    "print(\"Note: In a real environment, you would run the 'from importlib.metadata import version' code to get the actual version.\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- 3. Implement tiktoken Usage ---\n",
    "print(\"\\n--- Demonstrating tiktoken Usage ---\")\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "\n",
    "    # Instantiate the BPE tokenizer for \"gpt2\"\n",
    "    print(\"Instantiating tiktoken tokenizer for 'gpt2' model...\")\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "    # Example text as provided in the text\n",
    "    text = \"Hello, do you like tea? <|endoftext|> In the sunlit ter\"\n",
    "    print(f\"\\nOriginal text for encoding: '{text}'\")\n",
    "\n",
    "    # Encode the text, allowing the special <|endoftext|> token\n",
    "    print(\"Encoding text...\")\n",
    "    integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    print(f\"Encoded Token IDs: {integers}\")\n",
    "\n",
    "    # Verify observations as per the text:\n",
    "    # 1. Large ID for <|endoftext|>\n",
    "    # 2. Handles unknown words (like 'Hello') without <|unk|>\n",
    "    endoftext_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "    print(f\"\\nObservation 1: The ID for '<|endoftext|>' is {endoftext_token_id}\")\n",
    "    print(f\"(This is {endoftext_token_id}, which is a relatively large ID, indicating its position in the vocabulary.)\")\n",
    "\n",
    "    # To demonstrate point 2 (handling unknown words without <|unk|>):\n",
    "    # The word \"Hello,\" is not a common subword unit, and BPE breaks it down.\n",
    "    # We can't easily show the *absence* of <|unk|> in the output directly\n",
    "    # without knowing the specific subword tokens for \"Hello,\".\n",
    "    # The key point is that `tiktoken` *doesn't produce* <|unk|> for new words;\n",
    "    # it breaks them into known byte-pair-encoded subword units.\n",
    "\n",
    "    # Decode the token IDs back to text\n",
    "    print(\"\\nDecoding token IDs back to text...\")\n",
    "    strings = tokenizer.decode(integers)\n",
    "    print(f\"Decoded text: '{strings}'\")\n",
    "\n",
    "    print(\"\\nObservation 2: The BPE tokenizer handles unknown words (like 'Hello,') by breaking them into known subword units, rather than using an <|unk|> token. The decoded text closely matches the original, indicating successful reconstruction even for segments that might not be full dictionary words.\")\n",
    "    print(\"\\n(Note: The decoded text might have minor differences due to BPE's subword nature, but the overall meaning is preserved and no <|unk|> token appears.)\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nError: 'tiktoken' library not found. Please run 'pip install tiktoken' first.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9603319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "NLTK-based vocabulary size: 25\n",
      "ID for <|unk|>: 4\n",
      "ID for <|endoftext|>: 3\n",
      "----------------------------------------------------------------------\n",
      "Original Text: 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of palace.'\n",
      "\n",
      "Encoding text using NLTK-based tokenizer...\n",
      "Encoded Token IDs (NLTK): [4, 1, 10, 24, 14, 20, 5, 4, 4, 4, 7, 22, 19, 21, 15, 4, 2]\n",
      "\n",
      "Expected ID for <|unk|>: 4\n",
      "Expected ID for <|endoftext|>: 3\n",
      "Count of <|unk|> tokens in output: 5\n",
      "Count of <|endoftext|> tokens in output: 0\n",
      "\n",
      "(Note: 'Hello' and 'palace' are mapped to <|unk|> ID. Punctuation like ',' and '.' are separate tokens.)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- De-tokenizing for a quick sanity check (NLTK-based) ---\n",
      "De-tokenized Text (NLTK): '<|unk|> , do you like tea ? <|unk|> <|unk|> <|unk|> In the sunlit terraces of <|unk|> .'\n",
      "\n",
      "--- Observations for NLTK-based Tokenizer ---\n",
      "1.  **Punctuation Handling:** NLTK's `word_tokenize` separates punctuation (e.g., 'Hello,' becomes 'Hello', ','). This is a key difference from `tiktoken`'s BPE, which often keeps punctuation attached or handles it at a subword level.\n",
      "2.  **Unknown Words:** Words not in the vocabulary ('Hello', 'palace') are explicitly replaced by the `<|unk|>` token ID, and then decoded back to the string '<|unk|>'. This contrasts with BPE, which breaks unknown words into known subword units.\n",
      "3.  **Special Tokens:** The `<|endoftext|>` token is treated as a regular word token if it's explicitly added to the vocabulary, and its assigned ID is used.\n",
      "4.  **Subword vs. Word:** NLTK is primarily a word-level tokenizer. It does not perform subword tokenization like BPE. This means it either recognizes a full word or marks it as unknown (or splits it into smaller known words/punctuation).\n",
      "\n",
      "This implementation demonstrates how a tokenizer might be built using NLTK, highlighting its characteristics compared to the `tiktoken` BPE tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- 0. Download NLTK data (if not already downloaded) ---\n",
    "# NLTK tokenizers often require specific data files.\n",
    "# The 'punkt' tokenizer data is needed for word_tokenize.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer data...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"Download complete.\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- 1. Define NLTKBasedTokenizer Class ---\n",
    "# This class simulates a tokenizer using NLTK's word_tokenize\n",
    "# and incorporates handling for unknown words with a special <|unk|> token.\n",
    "class NLTKBasedTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        # Create token-to-ID and ID-to-token mappings\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
    "        \n",
    "        # Define the unknown token and ensure it's in the vocabulary\n",
    "        self.unk_token = \"<|unk|>\"\n",
    "        if self.unk_token not in self.str_to_int:\n",
    "            raise ValueError(f\"Vocabulary must contain the '{self.unk_token}' token.\")\n",
    "        self.unk_token_id = self.str_to_int[self.unk_token]\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encodes a text string into a list of token IDs using NLTK's word_tokenize.\n",
    "        Unknown words are mapped to the <|unk|> token ID.\n",
    "        \"\"\"\n",
    "        # Use NLTK's word_tokenize to split the text into tokens.\n",
    "        # This function separates words from punctuation (e.g., \"Hello,\" -> ['Hello', ',']).\n",
    "        nltk_tokens = word_tokenize(text)\n",
    "        \n",
    "        encoded_ids = []\n",
    "        for token in nltk_tokens:\n",
    "            # Look up the token in the vocabulary. If not found, use the <|unk|> ID.\n",
    "            encoded_ids.append(self.str_to_int.get(token, self.unk_token_id))\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs back into a text string.\n",
    "        A simple space join is used, which might not perfectly reconstruct original spacing\n",
    "        around punctuation but serves the conceptual purpose.\n",
    "        \"\"\"\n",
    "        tokens = [self.int_to_str[id] for id in ids]\n",
    "        # Join tokens with spaces. NLTK's detokenization is more complex\n",
    "        # to perfectly reverse word_tokenize, but this is sufficient for demo.\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# --- 2. Simulate `vocab` for NLTK based tokenizer ---\n",
    "# This vocabulary needs to be carefully constructed to reflect how NLTK tokenizes\n",
    "# (i.e., including common punctuation as separate tokens)\n",
    "# and to ensure 'Hello' and 'palace' are treated as unknown words.\n",
    "\n",
    "# Base words and punctuation that would typically be in a vocabulary\n",
    "# We ensure 'Hello' and 'palace' are NOT in this list so they become <|unk|>.\n",
    "base_words_for_nltk = [\n",
    "    \"It's\", 'the', 'last', 'he', 'painted', ',', 'you', 'know', '.', '\"', 'Mrs.', 'Gisburn', 'said', 'with', 'p',\n",
    "    'do', 'like', 'tea', '?', 'In', 'sunlit', 'terraces', 'of',\n",
    "    # Add special tokens as per the problem description\n",
    "    \"<|endoftext|>\", \"<|unk|>\"\n",
    "]\n",
    "\n",
    "# Create a sorted unique list of tokens to form the vocabulary\n",
    "all_unique_nltk_tokens = sorted(list(set(base_words_for_nltk)))\n",
    "\n",
    "# Create the vocabulary dictionary (token string to integer ID mapping).\n",
    "# We assign sequential IDs for simplicity.\n",
    "vocab_nltk = {token: integer for integer, token in enumerate(all_unique_nltk_tokens)}\n",
    "\n",
    "print(f\"NLTK-based vocabulary size: {len(vocab_nltk)}\")\n",
    "print(f\"ID for <|unk|>: {vocab_nltk.get('<|unk|>')}\")\n",
    "print(f\"ID for <|endoftext|>: {vocab_nltk.get('<|endoftext|>')}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# --- 3. Demonstrate Tokenization with NLTKBasedTokenizer ---\n",
    "tokenizer_nltk = NLTKBasedTokenizer(vocab_nltk)\n",
    "\n",
    "# The sample text to tokenize, including words that will be unknown ('Hello', 'palace')\n",
    "# and the special <|endoftext|> token.\n",
    "text_to_tokenize = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of palace.\"\n",
    "\n",
    "print(f\"Original Text: '{text_to_tokenize}'\")\n",
    "\n",
    "# Encode the text using our NLTK-based tokenizer\n",
    "print(\"\\nEncoding text using NLTK-based tokenizer...\")\n",
    "nltk_token_ids = tokenizer_nltk.encode(text_to_tokenize)\n",
    "print(f\"Encoded Token IDs (NLTK): {nltk_token_ids}\")\n",
    "\n",
    "# Verify the counts of special tokens in the output\n",
    "unk_id_nltk = vocab_nltk.get('<|unk|>')\n",
    "endoftext_id_nltk = vocab_nltk.get('<|endoftext|>')\n",
    "\n",
    "print(f\"\\nExpected ID for <|unk|>: {unk_id_nltk}\")\n",
    "print(f\"Expected ID for <|endoftext|>: {endoftext_id_nltk}\")\n",
    "\n",
    "print(f\"Count of <|unk|> tokens in output: {nltk_token_ids.count(unk_id_nltk)}\")\n",
    "print(f\"Count of <|endoftext|> tokens in output: {nltk_token_ids.count(endoftext_id_nltk)}\")\n",
    "\n",
    "print(\"\\n(Note: 'Hello' and 'palace' are mapped to <|unk|> ID. Punctuation like ',' and '.' are separate tokens.)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- 4. De-tokenize for a sanity check ---\n",
    "print(\"\\n--- De-tokenizing for a quick sanity check (NLTK-based) ---\")\n",
    "decoded_text_nltk = tokenizer_nltk.decode(nltk_token_ids)\n",
    "print(f\"De-tokenized Text (NLTK): '{decoded_text_nltk}'\")\n",
    "\n",
    "print(\"\\n--- Observations for NLTK-based Tokenizer ---\")\n",
    "print(\"1.  **Punctuation Handling:** NLTK's `word_tokenize` separates punctuation (e.g., 'Hello,' becomes 'Hello', ','). This is a key difference from `tiktoken`'s BPE, which often keeps punctuation attached or handles it at a subword level.\")\n",
    "print(\"2.  **Unknown Words:** Words not in the vocabulary ('Hello', 'palace') are explicitly replaced by the `<|unk|>` token ID, and then decoded back to the string '<|unk|>'. This contrasts with BPE, which breaks unknown words into known subword units.\")\n",
    "print(\"3.  **Special Tokens:** The `<|endoftext|>` token is treated as a regular word token if it's explicitly added to the vocabulary, and its assigned ID is used.\")\n",
    "print(\"4.  **Subword vs. Word:** NLTK is primarily a word-level tokenizer. It does not perform subword tokenization like BPE. This means it either recognizes a full word or marks it as unknown (or splits it into smaller known words/punctuation).\")\n",
    "print(\"\\nThis implementation demonstrates how a tokenizer might be built using NLTK, highlighting its characteristics compared to the `tiktoken` BPE tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20949f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer...\n",
      "BPE training complete. Final vocabulary size: 28\n",
      "\n",
      "BPE Tokenizer Training Complete.\n",
      "Final Vocab Size: 28\n",
      "\n",
      "--- Testing Encoding and Decoding ---\n",
      "Original Text: 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of palace.'\n",
      "Encoded IDs: [27, 4, 4, 5, 6, 7, 5, 8, 5, 9, 4, 10, 11, 3, 12, 3, 13, 14, 1, 1, 3, 15, 7, 5, 19, 12, 3, 25, 12, 1, 1, 10, 15, 12, 27, 16, 9, 15, 4, 10, 12, 12, 3, 17, 17, 13, 18, 3, 16, 5, 19, 21, 13, 4, 13, 18, 3, 22]\n",
      "Decoded Text: 'hello,doyouliketea?<|unk|><|unk|>endoftext<|unk|><|unk|>inthesunlitterracesofpalace.'\n",
      "\n",
      "--- Observations for BPE Tokenizer (from Scratch) ---\n",
      "1. **Handling Out-of-Vocabulary (OOV) Words:** Instead of a single <|unk|> token for a whole unknown word, BPE breaks words like 'Hello' or 'palace' down into smaller, known subword units (or individual characters) from its learned vocabulary. Only if even a single character isn't part of the initial character vocabulary, will <|unk|> appear.\n",
      "   - In this simple implementation, if a full segment or sub-segment isn't found, it defaults to <|unk|>.\n",
      "2. **Large Token IDs:** Special tokens or frequently merged tokens (subwords) often get higher IDs because they are added to the vocabulary later in the training process.\n",
      "3. **Subword Representation:** Observe how words are represented by combinations of smaller tokens. For example, 'terraces' might be tokenized as 'terr' + 'aces' if 'terraces' itself wasn't a merged token but those sub-parts were.\n",
      "4. **No Explicit Spaces in Decoded Output:** A basic BPE decoder just concatenates subword tokens. This often means spaces between original words are lost unless a special mechanism (like a leading space character in the token) is used. My `decode` has a `.replace(, )` that tries to put spaces back, but it's not perfect.\n",
      "\n",
      "(Note: This is a fundamental BPE implementation. Production-ready tokenizers like `tiktoken` handle nuances like byte-level encoding, whitespace preservation, and optimized lookup for performance and perfect round-trip capabilities.)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, vocab_size=500):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # Stores (pair_tuple) -> new_token_string\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # Special tokens (can be customized)\n",
    "        self.special_tokens = [\"<|endoftext|>\", \"<|unk|>\"]\n",
    "        self.unk_token = \"<|unk|>\" # Store the unk token string\n",
    "\n",
    "        # Initialize special tokens in vocab immediately\n",
    "        self.current_token_id = 0\n",
    "        for stoken in self.special_tokens:\n",
    "            self.token_to_id[stoken] = self.current_token_id\n",
    "            self.id_to_token[self.current_token_id] = stoken\n",
    "            self.current_token_id += 1\n",
    "        \n",
    "        # Now unk_token_id is guaranteed to be set\n",
    "        self.unk_token_id = self.token_to_id[self.unk_token]\n",
    "\n",
    "    def _get_stats(self, word_freqs):\n",
    "        \"\"\"Calculates the frequency of each adjacent pair in the current words.\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in word_freqs.items():\n",
    "            symbols = word.split(' ') # Words are represented as space-separated characters/tokens\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_pair(self, word_freqs, pair_to_merge, new_token):\n",
    "        \"\"\"Merges a given pair into a new token across all words.\"\"\"\n",
    "        merged_word_freqs = {}\n",
    "        # Ensure the pair is escaped for regex\n",
    "        bigram_str = re.escape(' '.join(pair_to_merge)) \n",
    "        # Use word boundaries for replacement to avoid partial matches within existing tokens\n",
    "        # Example: if merging 'e' and 'a' to 'ea', don't merge 'great' if 'ea' is inside.\n",
    "        # This regex ensures we only merge where the tokens are separated by spaces.\n",
    "        pattern = re.compile(r'(?<!\\S)' + bigram_str + r'(?!\\S)') \n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            if bigram_str in word:\n",
    "                merged_word = re.sub(pattern, new_token, word)\n",
    "                merged_word_freqs[merged_word] = freq\n",
    "            else:\n",
    "                merged_word_freqs[word] = freq\n",
    "        return merged_word_freqs\n",
    "\n",
    "\n",
    "    def train(self, text_corpus):\n",
    "        # 1. Initialize vocabulary with all unique characters from the corpus\n",
    "        # (excluding special tokens already added in __init__)\n",
    "        \n",
    "        # First, split the corpus into \"segments\" (words and punctuation)\n",
    "        # using a simple regex that keeps punctuation attached to words initially.\n",
    "        initial_segments = re.findall(r'\\b\\w+\\b|[^\\s\\w]+', text_corpus.lower())\n",
    "        \n",
    "        # Create initial character-level representation for each segment\n",
    "        initial_token_freqs = Counter()\n",
    "        for segment in initial_segments:\n",
    "            # Add new characters to vocab if not already special tokens\n",
    "            for char in segment:\n",
    "                if char not in self.token_to_id:\n",
    "                    self.token_to_id[char] = self.current_token_id\n",
    "                    self.id_to_token[self.current_token_id] = char\n",
    "                    self.current_token_id += 1\n",
    "            initial_token_freqs[' '.join(list(segment))] += 1 # Store as space-separated string\n",
    "\n",
    "        # Current set of \"words\" to be merged (space-separated characters)\n",
    "        current_word_freqs = initial_token_freqs\n",
    "\n",
    "        # Iteratively merge pairs\n",
    "        # We start from current_token_id because special tokens are already added.\n",
    "        while self.current_token_id < self.vocab_size:\n",
    "            pairs = self._get_stats(current_word_freqs)\n",
    "            \n",
    "            if not pairs:\n",
    "                break # No more pairs to merge\n",
    "\n",
    "            # Find the most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Create a new token from the merged pair\n",
    "            new_token = ''.join(best_pair)\n",
    "            \n",
    "            # If the new token already exists or adding it would exceed vocab_size, stop.\n",
    "            if new_token in self.token_to_id or self.current_token_id >= self.vocab_size:\n",
    "                break\n",
    "\n",
    "            # Add the new token to vocabulary\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.token_to_id[new_token] = self.current_token_id\n",
    "            self.id_to_token[self.current_token_id] = new_token\n",
    "            self.current_token_id += 1\n",
    "\n",
    "            # Update the word frequencies by merging the best pair\n",
    "            current_word_freqs = self._merge_pair(current_word_freqs, best_pair, new_token)\n",
    "            \n",
    "            # Print progress (optional)\n",
    "            # print(f\"Merge {len(self.merges)}: Merged {best_pair} into '{new_token}' (Freq: {pairs[best_pair]}), Vocab size: {self.current_token_id}\")\n",
    "\n",
    "        print(f\"BPE training complete. Final vocabulary size: {self.current_token_id}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Initial segmentation using the same logic as training\n",
    "        segments = re.findall(r'\\b\\w+\\b|[^\\s\\w]+', text.lower())\n",
    "        encoded_ids = []\n",
    "\n",
    "        for segment in segments:\n",
    "            # Greedily apply the largest possible known tokens.\n",
    "            # Start with characters as initial tokens for the segment.\n",
    "            current_subtokens = list(segment) \n",
    "            \n",
    "            # Apply merges learned during training\n",
    "            # This is a simplified application of merges. A more robust encoder would\n",
    "            # apply merges iteratively on `current_subtokens` until no more merges are possible.\n",
    "            # For this demo, we'll try to find the longest matches.\n",
    "            \n",
    "            processed_segment_ids = []\n",
    "            i = 0\n",
    "            while i < len(current_subtokens):\n",
    "                found_match = False\n",
    "                # Try to find the longest possible match starting from current position `i`\n",
    "                # Iterate from longest possible token down to a single character\n",
    "                for j in range(len(current_subtokens), i, -1):\n",
    "                    sub_token_str = \"\".join(current_subtokens[i:j])\n",
    "                    if sub_token_str in self.token_to_id:\n",
    "                        processed_segment_ids.append(self.token_to_id[sub_token_str])\n",
    "                        i = j # Move pointer past the matched token\n",
    "                        found_match = True\n",
    "                        break\n",
    "                \n",
    "                if not found_match:\n",
    "                    # If no known sub-token (even a single character) was found, it's truly unknown\n",
    "                    # This should ideally not happen if all characters are in the initial vocab.\n",
    "                    # Fallback to UNK token.\n",
    "                    processed_segment_ids.append(self.unk_token_id)\n",
    "                    i += 1 # Move past the single unknown character\n",
    "            \n",
    "            encoded_ids.extend(processed_segment_ids)\n",
    "\n",
    "        return encoded_ids\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        decoded_tokens = []\n",
    "        for id_val in ids:\n",
    "            if id_val in self.id_to_token:\n",
    "                decoded_tokens.append(self.id_to_token[id_val])\n",
    "            else:\n",
    "                # Fallback if ID is somehow not in the id_to_token map\n",
    "                decoded_tokens.append(self.unk_token) \n",
    "        \n",
    "        # A simple join will append tokens without considering original spacing.\n",
    "        # This will often result in words being \"squashed\" together if they were\n",
    "        # separated by spaces but then tokenized to adjacent BPE units.\n",
    "        # A full BPE detokenizer needs to be aware of original spaces.\n",
    "        # For conceptual demo, this is sufficient.\n",
    "        return \"\".join(decoded_tokens).replace(\" \", \" \") # Replace internal spaces if any were merged\n",
    "\n",
    "\n",
    "# --- Demonstration ---\n",
    "# Sample corpus for training (larger corpus yields better BPE merges)\n",
    "corpus = \"\"\"\n",
    "Hello, do you like tea? In the sunlit terraces of a grand palace,\n",
    "she found solace. The sun shone brightly on the palace walls.\n",
    "This is an example text to demonstrate byte pair encoding.\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate and train the tokenizer\n",
    "# A small vocab_size for demonstration purposes, to observe the merges.\n",
    "# For practical LLMs, vocab_size is usually tens of thousands (e.g., 50257 for GPT-2).\n",
    "bpe_tokenizer = SimpleBPETokenizer(vocab_size=100) # Reduced vocab_size for more merges to be visible\n",
    "print(\"Training BPE tokenizer...\")\n",
    "bpe_tokenizer.train(corpus)\n",
    "print(\"\\nBPE Tokenizer Training Complete.\")\n",
    "print(f\"Final Vocab Size: {len(bpe_tokenizer.token_to_id)}\")\n",
    "# print(\"Sample vocab (last 10):\", list(bpe_tokenizer.token_to_id.items())[-10:])\n",
    "\n",
    "# Test encoding and decoding\n",
    "print(\"\\n--- Testing Encoding and Decoding ---\")\n",
    "# The example text from the prompt, which might contain words that get broken down.\n",
    "test_text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of palace.\"\n",
    "print(f\"Original Text: '{test_text}'\")\n",
    "\n",
    "encoded_ids = bpe_tokenizer.encode(test_text)\n",
    "print(f\"Encoded IDs: {encoded_ids}\")\n",
    "\n",
    "decoded_text = bpe_tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded Text: '{decoded_text}'\")\n",
    "\n",
    "print(\"\\n--- Observations for BPE Tokenizer (from Scratch) ---\")\n",
    "print(\"1. **Handling Out-of-Vocabulary (OOV) Words:** Instead of a single <|unk|> token for a whole unknown word, BPE breaks words like 'Hello' or 'palace' down into smaller, known subword units (or individual characters) from its learned vocabulary. Only if even a single character isn't part of the initial character vocabulary, will <|unk|> appear.\")\n",
    "print(\"   - In this simple implementation, if a full segment or sub-segment isn't found, it defaults to <|unk|>.\")\n",
    "print(\"2. **Large Token IDs:** Special tokens or frequently merged tokens (subwords) often get higher IDs because they are added to the vocabulary later in the training process.\")\n",
    "print(\"3. **Subword Representation:** Observe how words are represented by combinations of smaller tokens. For example, 'terraces' might be tokenized as 'terr' + 'aces' if 'terraces' itself wasn't a merged token but those sub-parts were.\")\n",
    "print(\"4. **No Explicit Spaces in Decoded Output:** A basic BPE decoder just concatenates subword tokens. This often means spaces between original words are lost unless a special mechanism (like a leading space character in the token) is used. My `decode` has a `.replace(\" \", \" \")` that tries to put spaces back, but it's not perfect.\")\n",
    "print(\"\\n(Note: This is a fundamental BPE implementation. Production-ready tokenizers like `tiktoken` handle nuances like byte-level encoding, whitespace preservation, and optimized lookup for performance and perfect round-trip capabilities.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082d989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
