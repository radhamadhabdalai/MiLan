{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e64b7-4651-4b06-bdd8-398661bbaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:40:19.670044: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-11 10:40:19.750406: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-11 10:40:20.332318: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-11 10:40:20.336666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-11 10:40:21.385697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "Copyright (c) 2023, 2024 , Prof. Radhamadhab Dalai, ITER , Siksha O Aanusandhan University, \n",
    "Odisha, India\n",
    "Author's email address :  radhamadhabdalai@soa.ac.in\n",
    "#####################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def xywh_to_x1x2y1y2(box):\n",
    "    xy = box[..., 0:2]\n",
    "    wh = box[..., 2:4]\n",
    "\n",
    "    x1y1 = xy - wh / 2\n",
    "    x2y2 = xy + wh / 2\n",
    "\n",
    "    y_box = tf.concat([x1y1, x2y2], axis=-1)\n",
    "    return y_box\n",
    "\n",
    "\n",
    "def xywh_to_y1x1y2x2(box):\n",
    "    x = box[..., 0:1]\n",
    "    y = box[..., 1:2]\n",
    "    w = box[..., 2:3]\n",
    "    h = box[..., 3:4]\n",
    "\n",
    "    yx = tf.concat([y, x], axis=-1)\n",
    "    hw = tf.concat([h, w], axis=-1)\n",
    "\n",
    "    y1x1 = yx - hw / 2\n",
    "    y2x2 = yx + hw / 2\n",
    "\n",
    "    y_box = tf.concat([y1x1, y2x2], axis=-1)\n",
    "    return y_box\n",
    "\n",
    "\n",
    "def broadcast_iou(box_a, box_b):\n",
    "    \"\"\"\n",
    "    calculate iou between box_a and multiple box_b in a broadcast way.\n",
    "    Used this implementation as reference: \n",
    "    https://github.com/dmlc/gluon-cv/blob/c3dd20d4b1c1ef8b7d381ad2a7d04a68c5fa1221/gluoncv/nn/bbox.py#L206\n",
    "\n",
    "    inputs:\n",
    "    box_a: a tensor full of boxes, eg. (B, N, 4), box is in x1y1x2y2\n",
    "    box_b: another tensor full of boxes, eg. (B, M, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # (B, N, 1, 4)\n",
    "    box_a = tf.expand_dims(box_a, -2)\n",
    "    # (B, 1, M, 4)\n",
    "    box_b = tf.expand_dims(box_b, -3)\n",
    "    # (B, N, M, 4)\n",
    "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_a), tf.shape(box_b))\n",
    "\n",
    "    # (B, N, M, 4)\n",
    "    # (B, N, M, 4)\n",
    "    box_a = tf.broadcast_to(box_a, new_shape)\n",
    "    box_b = tf.broadcast_to(box_b, new_shape)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    al, at, ar, ab = tf.split(box_a, 4, -1)\n",
    "    bl, bt, br, bb = tf.split(box_b, 4, -1)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    left = tf.math.maximum(al, bl)\n",
    "    right = tf.math.minimum(ar, br)\n",
    "    top = tf.math.maximum(at, bt)\n",
    "    bot = tf.math.minimum(ab, bb)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    iw = tf.clip_by_value(right - left, 0, 1)\n",
    "    ih = tf.clip_by_value(bot - top, 0, 1)\n",
    "    i = iw * ih\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    area_a = (ar - al) * (ab - at)\n",
    "    area_b = (br - bl) * (bb - bt)\n",
    "    union = area_a + area_b - i\n",
    "\n",
    "    # (B, N, M)\n",
    "    iou = tf.squeeze(i / (union + 1e-7), axis=-1)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def binary_cross_entropy(logits, labels):\n",
    "    epsilon = 1e-7\n",
    "    logits = tf.clip_by_value(logits, epsilon, 1 - epsilon)\n",
    "    return -(labels * tf.math.log(logits) +\n",
    "             (1 - labels) * tf.math.log(1 - logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa9edd3-f6b6-404d-a1e0-24cb16096bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from utils import broadcast_iou, xywh_to_x1x2y1y2\n",
    "\n",
    "\n",
    "class Postprocessor(object):\n",
    "    def __init__(self, iou_thresh, score_thresh, max_detection=100):\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.score_thresh = score_thresh\n",
    "        self.max_detection = max_detection\n",
    "\n",
    "    def __call__(self, raw_yolo_outputs):\n",
    "        boxes, objectness, class_probs = [], [], []\n",
    "\n",
    "        for o in raw_yolo_outputs:\n",
    "            batch_size = tf.shape(o[0])[0]\n",
    "            num_classes = tf.shape(o[2])[-1]\n",
    "            # needs to translate from xywh to y1x1y2x2 format\n",
    "            boxes.append(tf.reshape(o[0], (batch_size, -1, 4)))\n",
    "            objectness.append(tf.reshape(o[1], (batch_size, -1, 1)))\n",
    "            class_probs.append(tf.reshape(o[2], (batch_size, -1, num_classes)))\n",
    "\n",
    "        boxes = xywh_to_x1x2y1y2(tf.concat(boxes, axis=1))\n",
    "\n",
    "        objectness = tf.concat(objectness, axis=1)\n",
    "        class_probs = tf.concat(class_probs, axis=1)\n",
    "\n",
    "        scores = objectness\n",
    "        scores = tf.reshape(scores,\n",
    "                            (tf.shape(scores)[0], -1, tf.shape(scores)[-1]))\n",
    "\n",
    "        final_boxes, final_scores, final_classes, valid_detections = self.batch_non_maximum_suppression(\n",
    "            boxes, scores, class_probs, self.iou_thresh, self.score_thresh,\n",
    "            self.max_detection)\n",
    "\n",
    "        return final_boxes, final_scores, final_classes, valid_detections\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_non_maximum_suppression(boxes, scores, classes, iou_threshold,\n",
    "                                      score_threshold, max_detection):\n",
    "        \"\"\"\n",
    "        Unlike tf.image.combined_non_max_suppression, we are making multi-label classification on the detection\n",
    "        \"\"\"\n",
    "\n",
    "        def single_batch_nms(candidate_boxes):\n",
    "            # filter out predictions with score less than score_threshold\n",
    "            candidate_boxes = tf.boolean_mask(\n",
    "                candidate_boxes, candidate_boxes[..., 4] >= score_threshold)\n",
    "            outputs = tf.zeros((max_detection + 1,\n",
    "                                tf.shape(candidate_boxes)[-1]))\n",
    "            indices = []\n",
    "            updates = []\n",
    "\n",
    "            count = 0\n",
    "            # keep running this until there's no more candidate box or max_detection is met\n",
    "            while tf.shape(candidate_boxes)[0] > 0 and count < max_detection:\n",
    "                # pick the box with the highest score\n",
    "                best_idx = tf.math.argmax(candidate_boxes[..., 4], axis=0)\n",
    "                best_box = candidate_boxes[best_idx]\n",
    "                # add this best box to the output\n",
    "                indices.append([count])\n",
    "                updates.append(best_box)\n",
    "                count += 1\n",
    "                # remove this box from candidate boxes\n",
    "                candidate_boxes = tf.concat([\n",
    "                    candidate_boxes[0:best_idx],\n",
    "                    candidate_boxes[best_idx + 1:tf.shape(candidate_boxes)[0]]\n",
    "                ],\n",
    "                                            axis=0)\n",
    "                # calculate IOU between this box and all remaining candidate boxes\n",
    "                iou = broadcast_iou(best_box[0:4], candidate_boxes[..., 0:4])\n",
    "                # remove all candidate boxes with IOU bigger than iou_threshold\n",
    "                candidate_boxes = tf.boolean_mask(candidate_boxes,\n",
    "                                                  iou[0] <= iou_threshold)\n",
    "            if count > 0:\n",
    "                # also append num_detection to the result\n",
    "                count_index = [[max_detection]]\n",
    "                count_updates = [\n",
    "                    tf.fill([tf.shape(candidate_boxes)[-1]], count)\n",
    "                ]\n",
    "                indices = tf.concat([indices, count_index], axis=0)\n",
    "                updates = tf.concat([updates, count_updates], axis=0)\n",
    "                outputs = tf.tensor_scatter_nd_update(outputs, indices,\n",
    "                                                      updates)\n",
    "            return outputs\n",
    "\n",
    "        combined_boxes = tf.concat([boxes, scores, classes], axis=2)\n",
    "        result = tf.map_fn(single_batch_nms, combined_boxes)\n",
    "        # take out num_detection from the result\n",
    "        valid_counts = tf.expand_dims(\n",
    "            tf.map_fn(lambda x: x[max_detection][0], result), axis=-1)\n",
    "        final_result = tf.map_fn(lambda x: x[0:max_detection], result)\n",
    "        nms_boxes, nms_scores, nms_classes = tf.split(\n",
    "            final_result, [4, 1, -1], axis=-1)\n",
    "        return nms_boxes, nms_scores, nms_classes, tf.cast(\n",
    "            valid_counts, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748dd649-b63e-4765-9771-5d9a0a4a4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### ylov3\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import utils\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Input,\n",
    "    Lambda,\n",
    "    LeakyReLU,\n",
    "    MaxPool2D,\n",
    "    UpSampling2D,\n",
    "    ZeroPadding2D,\n",
    "    BatchNormalization,\n",
    ")\n",
    "# from utils import xywh_to_x1x2y1y2, xywh_to_y1x1y2x2, broadcast_iou, binary_cross_entropy\n",
    "\n",
    "anchors_wh = np.array([[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],\n",
    "                       [59, 119], [116, 90], [156, 198], [373, 326]],\n",
    "                      np.float32) / 416\n",
    "\n",
    "\n",
    "def DarknetConv(inputs, filters, kernel_size, strides, name):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        name=name + '_conv2d',\n",
    "        use_bias=False,\n",
    "        # kernel_regularizer=tf.keras.regularizers.l2(0.0005)\n",
    "    )(inputs)\n",
    "    # YoloV2:\n",
    "    # \"By adding batch normalization on all of the convolutional layers in\n",
    "    #  YOLO we get more than 2% improvement in mAP.\"\n",
    "    x = BatchNormalization(name=name + '_bn')(x)\n",
    "    # YoloV1:\n",
    "    # \"We use a linear activation function for the ﬁnal layer and all other\n",
    "    #  layers use the following leaky rectiﬁed linear activation\"\n",
    "    x = LeakyReLU(alpha=0.1, name=name + '_leakyrelu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def DarknetResidual(inputs, filters1, filters2, name):\n",
    "    shortcut = inputs\n",
    "    x = DarknetConv(\n",
    "        inputs, filters=filters1, kernel_size=1, strides=1, name=name + '_1x1')\n",
    "    x = DarknetConv(\n",
    "        x, filters=filters2, kernel_size=3, strides=1, name=name + '_3x3')\n",
    "    x = Add(name=name + '_add')([shortcut, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def Darknet(shape=(256, 256, 3)):\n",
    "    # YoloV3:\n",
    "    # Table 1. Darknet-53.\n",
    "    inputs = Input(shape=shape)\n",
    "\n",
    "    x = DarknetConv(inputs, 32, kernel_size=3, strides=1, name='conv2d_0')\n",
    "\n",
    "    x = DarknetConv(x, 64, kernel_size=3, strides=2, name='conv2d_1')\n",
    "    # 1x residual blocks\n",
    "    for i in range(1):\n",
    "        x = DarknetResidual(x, 32, 64, 'residual_0_' + str(i))\n",
    "\n",
    "    x = DarknetConv(x, 128, kernel_size=3, strides=2, name='conv2d_2')\n",
    "    # 2x residual blocks\n",
    "    for i in range(2):\n",
    "        x = DarknetResidual(x, 64, 128, 'residual_1_' + str(i))\n",
    "\n",
    "    x = DarknetConv(x, 256, kernel_size=3, strides=2, name='conv2d_3')\n",
    "    # 8x residual blocks\n",
    "    for i in range(8):\n",
    "        x = DarknetResidual(x, 128, 256, 'residual_2_' + str(i))\n",
    "\n",
    "    y0 = x\n",
    "\n",
    "    x = DarknetConv(x, 512, kernel_size=3, strides=2, name='conv2d_4')\n",
    "    # 8x residual blocks\n",
    "    for i in range(8):\n",
    "        x = DarknetResidual(x, 256, 512, 'residual_3_' + str(i))\n",
    "\n",
    "    y1 = x\n",
    "\n",
    "    x = DarknetConv(x, 1024, kernel_size=3, strides=2, name='conv2d_5')\n",
    "    # 4x residual blocks\n",
    "    for i in range(4):\n",
    "        x = DarknetResidual(x, 512, 1024, 'residual_4_' + str(i))\n",
    "\n",
    "    y2 = x\n",
    "\n",
    "    return tf.keras.Model(inputs, (y0, y1, y2), name='darknet_53')\n",
    "\n",
    "\n",
    "def YoloV3(shape=(416, 416, 3), num_classes=2, training=False):\n",
    "    # YoloV3:\n",
    "    # \"In our experiments with COCO [10] we predict 3 boxes at each scale so\n",
    "    #  the tensor is N × N × [3 ∗ (4 + 1 + 80)] for the 4 bounding box offsets,\n",
    "    #  1 objectness prediction, and 80 class predictions.\"\n",
    "    # 3 * (4 + 1 + num_classes) = 21\n",
    "    final_filters = 3 * (4 + 1 + num_classes)\n",
    "\n",
    "    inputs = Input(shape=shape)\n",
    "\n",
    "    backbone = Darknet(shape)\n",
    "    x_small, x_medium, x_large = backbone(inputs)\n",
    "\n",
    "    # large scale detection\n",
    "    # https://github.com/pjreddie/darknet/blob/61c9d02ec461e30d55762ec7669d6a1d3c356fb2/cfg/yolov3.cfg#L549-L788\n",
    "    x = DarknetConv(\n",
    "        x_large,\n",
    "        512,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        name='detector_scale_large_1x1_1')\n",
    "    x = DarknetConv(\n",
    "        x, 1024, kernel_size=3, strides=1, name='detector_scale_large_3x3_1')\n",
    "    x = DarknetConv(\n",
    "        x, 512, kernel_size=1, strides=1, name='detector_scale_large_1x1_2')\n",
    "    x = DarknetConv(\n",
    "        x, 1024, kernel_size=3, strides=1, name='detector_scale_large_3x3_2')\n",
    "    x = DarknetConv(\n",
    "        x, 512, kernel_size=1, strides=1, name='detector_scale_large_1x1_3')\n",
    "\n",
    "    y_large = DarknetConv(\n",
    "        x, 1024, kernel_size=3, strides=1, name='detector_scale_large_3x3_3')\n",
    "    y_large = Conv2D(\n",
    "        filters=final_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        name='detector_scale_large_final_conv2d',\n",
    "    )(y_large)\n",
    "\n",
    "    \n",
    "    x, 256, kernel_size=1, strides=1, name='detector_scale_medium_1x1_0')\n",
    "    x = UpSampling2D(size=(2, 2), name='detector_scale_1_upsampling')(x)\n",
    "    x = Concatenate(name='detector_scale_1_concat')([x, x_medium])\n",
    "\n",
    "    x = DarknetConv(\n",
    "        x, 256, kernel_size=1, strides=1, name='detector_scale_medium_1x1_1')\n",
    "    x = DarknetConv(\n",
    "        x, 512, kernel_size=3, strides=1, name='detector_scale_medium_3x3_1')\n",
    "    x = DarknetConv(\n",
    "        x, 256, kernel_size=1, strides=1, name='detector_scale_medium_1x1_2')\n",
    "    x = DarknetConv(\n",
    "        x, 512, kernel_size=3, strides=1, name='detector_scale_medium_3x3_2')\n",
    "    x = DarknetConv(\n",
    "        x, 256, kernel_size=1, strides=1, name='detector_scale_medium_1x1_3')\n",
    "\n",
    "    y_medium = DarknetConv(\n",
    "        x, 512, kernel_size=3, strides=1, name='detector_scale_medium_3x3_3')\n",
    "    y_medium = Conv2D(\n",
    "        filters=final_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        name='detector_scale_medium_final_conv2d',\n",
    "    )(y_medium)\n",
    "\n",
    "   \n",
    "    x = DarknetConv(\n",
    "        x, 128, kernel_size=1, strides=1, name='detector_scale_small_1x1_0')\n",
    "    x = UpSampling2D(size=(2, 2), name='detector_scale_small_upsampling')(x)\n",
    "    x = Concatenate(name='detector_scale_small_concat')([x, x_small])\n",
    "\n",
    "    x = DarknetConv(\n",
    "        x, 128, kernel_size=1, strides=1, name='detector_scale_small_1x1_1')\n",
    "    x = DarknetConv(\n",
    "        x, 256, kernel_size=3, strides=1, name='detector_scale_small_3x3_1')\n",
    "    x = DarknetConv(\n",
    "        x, 128, kernel_size=1, strides=1, name='detector_scale_small_1x1_2')\n",
    "    x = DarknetConv(\n",
    "        x, 256, kernel_size=3, strides=1, name='detector_scale_small_3x3_2')\n",
    "    x = DarknetConv(\n",
    "        x, 128, kernel_size=1, strides=1, name='detector_scale_small_1x1_3')\n",
    "\n",
    "    y_small = DarknetConv(\n",
    "        x, 256, kernel_size=3, strides=1, name='detector_scale_small_3x3_3')\n",
    "    y_small = Conv2D(\n",
    "        filters=final_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        name='detector_scale_small_final_conv2d',\n",
    "    )(y_small)\n",
    "\n",
    "    # reshape (N, grid, grid, 21) into (N, grid, grid, 3, 7) to seprate predictions\n",
    "    # for each anchor\n",
    "    y_small_shape = tf.shape(y_small)\n",
    "    y_medium_shape = tf.shape(y_medium)\n",
    "    y_large_shape = tf.shape(y_large)\n",
    "\n",
    "    y_small = tf.reshape(\n",
    "        y_small, (y_small_shape[0], y_small_shape[1], y_small_shape[2], 3, -1),\n",
    "        name='detector_reshape_small')\n",
    "    y_medium = tf.reshape(\n",
    "        y_medium,\n",
    "        (y_medium_shape[0], y_medium_shape[1], y_medium_shape[2], 3, -1),\n",
    "        name='detector_reshape_meidum')\n",
    "    y_large = tf.reshape(\n",
    "        y_large, (y_large_shape[0], y_large_shape[1], y_large_shape[2], 3, -1),\n",
    "        name='detector_reshape_large')\n",
    "\n",
    "    if training:\n",
    "        return tf.keras.Model(inputs, (y_small, y_medium, y_large))\n",
    "\n",
    "    box_small = Lambda(\n",
    "        lambda x: get_absolute_yolo_box(x, anchors_wh[0:3], num_classes),\n",
    "        name='detector_final_box_small')(y_small)\n",
    "    box_medium = Lambda(\n",
    "        lambda x: get_absolute_yolo_box(x, anchors_wh[3:6], num_classes),\n",
    "        name='detector_final_box_medium')(y_medium)\n",
    "    box_large = Lambda(\n",
    "        lambda x: get_absolute_yolo_box(x, anchors_wh[6:9], num_classes),\n",
    "        name='detector_final_box_large')(y_large)\n",
    "\n",
    "    outputs = (box_small, box_medium, box_large)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def get_absolute_yolo_box(y_pred, valid_anchors_wh, num_classes):\n",
    "   \n",
    "\n",
    "    t_xy, t_wh, objectness, classes = tf.split(\n",
    "        y_pred, (2, 2, 1, num_classes), axis=-1)\n",
    "\n",
    "    objectness = tf.sigmoid(objectness)\n",
    "    classes = tf.sigmoid(classes)\n",
    "\n",
    "    grid_size = tf.shape(y_pred)[1]\n",
    "    # meshgrid generates a grid that repeats by given range. It's the Cx and Cy in YoloV3 paper.\n",
    "    # for example, tf.meshgrid(tf.range(3), tf.range(3)) will generate a list with two elements\n",
    "    # note that in real code, the grid_size should be something like 13, 26, 52 for examples here and below\n",
    "    #\n",
    "    # [[0, 1, 2],\n",
    "    #  [0, 1, 2],\n",
    "    #  [0, 1, 2]]\n",
    "    #\n",
    "    # [[0, 0, 0],\n",
    "    #  [1, 1, 1],\n",
    "    #  [2, 2, 2]]\n",
    "    #\n",
    "    C_xy = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "\n",
    "    # next, we stack two items in the list together in the last dimension, so that\n",
    "    # we can interleve these elements together and become this:\n",
    "    #\n",
    "    # [[[0, 0], [1, 0], [2, 0]],\n",
    "    #  [[0, 1], [1, 1], [2, 1]],\n",
    "    #  [[0, 2], [1, 2], [2, 2]]]\n",
    "    #\n",
    "    C_xy = tf.stack(C_xy, axis=-1)\n",
    "\n",
    "    # let's add an empty dimension at axis=2 to expand the tensor to this:\n",
    "    #\n",
    "    # [[[[0, 0]], [[1, 0]], [[2, 0]]],\n",
    "    #  [[[0, 1]], [[1, 1]], [[2, 1]]],\n",
    "    #  [[[0, 2]], [[1, 2]], [[2, 2]]]]\n",
    "    #\n",
    "    # at this moment, we now have a grid, which can always give us (y, x)\n",
    "    # if we access grid[x][y]. For example, grid[0][1] == [[1, 0]]\n",
    "    C_xy = tf.expand_dims(C_xy, axis=2)  # [gx, gy, 1, 2]\n",
    "\n",
    "    # YoloV2, YoloV3:\n",
    "    # bx = sigmoid(tx) + Cx\n",
    "    # by = sigmoid(ty) + Cy\n",
    "    #\n",
    "    # for example, if all elements in b_xy are (0.1, 0.2), the result will be\n",
    "    #\n",
    "    # [[[[0.1, 0.2]], [[1.1, 0.2]], [[2.1, 0.2]]],\n",
    "    #  [[[0.1, 1.2]], [[1.1, 1.2]], [[2.1, 1.2]]],\n",
    "    #  [[[0.1, 2.2]], [[1.1, 2.2]], [[2.1, 2.2]]]]\n",
    "    #\n",
    "    b_xy = tf.sigmoid(t_xy) + tf.cast(C_xy, tf.float32)\n",
    "\n",
    "    # finally, divide this absolute box_xy by grid_size, and then we will get the normalized bbox centroids\n",
    "    # for each anchor in each grid cell. b_xy is now in shape (batch_size, grid_size, grid_size, num_anchor, 2)\n",
    "    #\n",
    "    # [[[[0.1/3, 0.2/3]], [[1.1/3, 0.2/3]], [[2.1/3, 0.2/3]]],\n",
    "    #  [[[0.1/3, 1.2/3]], [[1.1/3, 1.2]/3], [[2.1/3, 1.2/3]]],\n",
    "    #  [[[0.1/3, 2.2/3]], [[1.1/3, 2.2/3]], [[2.1/3, 2.2/3]]]]\n",
    "    #\n",
    "    b_xy = b_xy / tf.cast(grid_size, tf.float32)\n",
    "\n",
    "    # YoloV2:\n",
    "    # \"If the cell is offset from the top left corner of the image by (cx , cy)\n",
    "    # and the bounding box prior has width and height pw , ph , then the predictions correspond to: \"\n",
    "    #\n",
    "    # https://github.com/pjreddie/darknet/issues/568#issuecomment-469600294\n",
    "    # \"It’s OK for the predicted box to be wider and/or taller than the original image, but\n",
    "    # it does not make sense for the box to have a negative width or height. That’s why\n",
    "    # we take the exponent of the predicted number.\"\n",
    "    b_wh = tf.exp(t_wh) * valid_anchors_wh\n",
    "\n",
    "    y_box = tf.concat([b_xy, b_wh], axis=-1)\n",
    "    return y_box, objectness, classes\n",
    "\n",
    "\n",
    "def get_relative_yolo_box(y_true, valid_anchors_wh):\n",
    "    \"\"\"\n",
    "    This is the inverse of `get_absolute_yolo_box` above. It's turning (bx, by, bw, bh) into\n",
    "    (tx, ty, tw, th) that is relative to cell location.\n",
    "    \"\"\"\n",
    "    grid_size = tf.shape(y_true)[1]\n",
    "    C_xy = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "    C_xy = tf.expand_dims(tf.stack(C_xy, axis=-1), axis=2)\n",
    "\n",
    "    b_xy = y_true[..., 0:2]\n",
    "    b_wh = y_true[..., 2:4]\n",
    "    t_xy = b_xy * tf.cast(grid_size, tf.float32) - tf.cast(C_xy, tf.float32)\n",
    "\n",
    "    t_wh = tf.math.log(b_wh / valid_anchors_wh)\n",
    "    # b_wh could have some cells are 0, divided by anchor could result in inf or nan\n",
    "    t_wh = tf.where(\n",
    "        tf.logical_or(tf.math.is_inf(t_wh), tf.math.is_nan(t_wh)),\n",
    "        tf.zeros_like(t_wh), t_wh)\n",
    "\n",
    "    y_box = tf.concat([t_xy, t_wh], axis=-1)\n",
    "    return y_box\n",
    "\n",
    "\n",
    "class YoloLoss(object):\n",
    "    def __init__(self, num_classes, valid_anchors_wh):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thresh = 0.5\n",
    "        self.valid_anchors_wh = valid_anchors_wh\n",
    "        self.lambda_coord = 5.0\n",
    "        self.lamda_noobj = 0.5\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        calculate the loss of model prediction for one scale\n",
    "        \"\"\"\n",
    "        # for xy and wh, I seperated them into two groups with different suffix\n",
    "        # suffix rel (relative) means that its coordinates are relative to cells\n",
    "        # basically (tx, ty, tw, th) format from the paper\n",
    "        # _rel is used to calcuate the loss\n",
    "        # suffix abs (absolute) means that its coordinates are absolute with in whole image\n",
    "        # basically (bx, by, bw, bh) format from the paper\n",
    "        # _abs is used to calcuate iou and ignore mask\n",
    "\n",
    "        # split y_pred into xy, wh, objectness and one-hot classes\n",
    "        # pred_xy_rel: (batch, grid, grid, anchor, 2)\n",
    "        # pred_wh_rel: (batch, grid, grid, anchor, 2)\n",
    "        # TODO: Add comment for the sigmoid here\n",
    "        pred_xy_rel = tf.sigmoid(y_pred[..., 0:2])\n",
    "        pred_wh_rel = y_pred[..., 2:4]\n",
    "\n",
    "        # this box is used to calculate iou, NOT loss. so we can't use\n",
    "        # cell offset anymore and have to transform it into true values\n",
    "        # both pred_obj and pred_class has been sigmoid'ed here\n",
    "        # pred_xy_abs: (batch, grid, grid, anchor, 2)\n",
    "        # pred_wh_abs: (batch, grid, grid, anchor, 2)\n",
    "        # pred_obj: (batch, grid, grid, anchor, 1)\n",
    "        # pred_class: (batch, grid, grid, anchor, num_classes)\n",
    "        pred_box_abs, pred_obj, pred_class = get_absolute_yolo_box(\n",
    "            y_pred, self.valid_anchors_wh, self.num_classes)\n",
    "        pred_box_abs = xywh_to_x1x2y1y2(pred_box_abs)\n",
    "\n",
    "        # split y_true into xy, wh, objectness and one-hot classes\n",
    "        # pred_xy_abs: (batch, grid, grid, anchor, 2)\n",
    "        # pred_wh_abs: (batch, grid, grid, anchor, 2)\n",
    "        # pred_obj: (batch, grid, grid, anchor, 1)\n",
    "        # pred_class: (batch, grid, grid, anchor, num_classes)\n",
    "        true_xy_abs, true_wh_abs, true_obj, true_class = tf.split(\n",
    "            y_true, (2, 2, 1, self.num_classes), axis=-1)\n",
    "        true_box_abs = tf.concat([true_xy_abs, true_wh_abs], axis=-1)\n",
    "        true_box_abs = xywh_to_x1x2y1y2(true_box_abs)\n",
    "\n",
    "        # true_box_rel: (batch, grid, grid, anchor, 4)\n",
    "        true_box_rel = get_relative_yolo_box(y_true, self.valid_anchors_wh)\n",
    "        true_xy_rel = true_box_rel[..., 0:2]\n",
    "        true_wh_rel = true_box_rel[..., 2:4]\n",
    "\n",
    "        # some adjustment to improve small box detection, note the (2-truth.w*truth.h) below\n",
    "        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L190\n",
    "        weight = 2 - true_wh_abs[..., 0] * true_wh_abs[..., 1]\n",
    "\n",
    "        # YoloV2:\n",
    "        # \"If the cell is offset from the top left corner of the image by (cx , cy)\n",
    "        # and the bounding box prior has width and height pw , ph , then the predictions correspond to:\"\n",
    "        #\n",
    "        # to calculate the iou and determine the ignore mask, we need to first transform\n",
    "        # prediction into real coordinates (bx, by, bw, bh)\n",
    "\n",
    "        # YoloV2:\n",
    "        # \"This ground truth value can be easily computed by inverting the equations above.\"\n",
    "        #\n",
    "        # to calculate loss and differentiation, we need to transform ground truth into\n",
    "        # cell offset first like demonstrated here:\n",
    "        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L93\n",
    "        xy_loss = self.calc_xy_loss(true_obj, true_xy_rel, pred_xy_rel, weight)\n",
    "        wh_loss = self.calc_wh_loss(true_obj, true_wh_rel, pred_wh_rel, weight)\n",
    "        class_loss = self.calc_class_loss(true_obj, true_class, pred_class)\n",
    "\n",
    "        # use the absolute yolo box to calculate iou and ignore mask\n",
    "        ignore_mask = self.calc_ignore_mask(true_obj, true_box_abs,\n",
    "                                            pred_box_abs)\n",
    "        obj_loss = self.calc_obj_loss(true_obj, pred_obj, ignore_mask)\n",
    "\n",
    "        # YoloV1: Function (3)\n",
    "        return xy_loss + wh_loss + class_loss + obj_loss, (xy_loss, wh_loss,\n",
    "                                                           class_loss,\n",
    "                                                           obj_loss)\n",
    "\n",
    "    def calc_ignore_mask(self, true_obj, true_box, pred_box):\n",
    "        # YOLOv3:\n",
    "        # \"If the bounding box prior is not the best but does overlap a ground\n",
    "        # truth object by more than some threshold we ignore the prediction,\n",
    "        # following [17]. We use the threshold of .5.\"\n",
    "        # calculate the iou for each pair of pred bbox and true bbox, then find the best among them\n",
    "\n",
    "        # (None, 13, 13, 3, 4)\n",
    "        true_box_shape = tf.shape(true_box)\n",
    "        # (None, 13, 13, 3, 4)\n",
    "        pred_box_shape = tf.shape(pred_box)\n",
    "        # (None, 507, 4)\n",
    "        true_box = tf.reshape(true_box, [true_box_shape[0], -1, 4])\n",
    "        # sort true_box to have non-zero boxes rank first\n",
    "        true_box = tf.sort(true_box, axis=1, direction=\"DESCENDING\")\n",
    "        # (None, 100, 4)\n",
    "        # only use maximum 100 boxes per groundtruth to calcualte IOU, otherwise\n",
    "        # GPU emory comsumption would explode for a matrix like (16, 52*52*3, 52*52*3, 4)\n",
    "        true_box = true_box[:, 0:100, :]\n",
    "        # (None, 507, 4)\n",
    "        pred_box = tf.reshape(pred_box, [pred_box_shape[0], -1, 4])\n",
    "\n",
    "        # https://github.com/dmlc/gluon-cv/blob/06bb7ec2044cdf3f433721be9362ab84b02c5a90/gluoncv/model_zoo/yolo/yolo_target.py#L198\n",
    "        # (None, 507, 507)\n",
    "        iou = broadcast_iou(pred_box, true_box)\n",
    "        # (None, 507)\n",
    "        best_iou = tf.reduce_max(iou, axis=-1)\n",
    "        # (None, 13, 13, 3)\n",
    "        best_iou = tf.reshape(best_iou, [pred_box_shape[0], pred_box_shape[1], pred_box_shape[2], pred_box_shape[3]])\n",
    "        # ignore_mask = 1 => don't ignore\n",
    "        # ignore_mask = 0 => should ignore\n",
    "        ignore_mask = tf.cast(best_iou < self.ignore_thresh, tf.float32)\n",
    "        # (None, 13, 13, 3, 1)\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, axis=-1)\n",
    "        return ignore_mask\n",
    "\n",
    "    def calc_obj_loss(self, true_obj, pred_obj, ignore_mask):\n",
    "        \"\"\"\n",
    "        calculate loss of objectness: sum of L2 distances\n",
    "\n",
    "        inputs:\n",
    "        true_obj: objectness from ground truth in shape of (batch, grid, grid, anchor, num_classes)\n",
    "        pred_obj: objectness from model prediction in shape of (batch, grid, grid, anchor, num_classes)\n",
    "\n",
    "        outputs:\n",
    "        obj_loss: objectness loss\n",
    "        \"\"\"\n",
    "        obj_entropy = binary_cross_entropy(pred_obj, true_obj)\n",
    "\n",
    "        obj_loss = true_obj * obj_entropy\n",
    "        noobj_loss = (1 - true_obj) * obj_entropy * ignore_mask\n",
    "\n",
    "        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3, 4))\n",
    "        noobj_loss = tf.reduce_sum(\n",
    "            noobj_loss, axis=(1, 2, 3, 4)) * self.lamda_noobj\n",
    "\n",
    "        return obj_loss + noobj_loss\n",
    "\n",
    "    def calc_class_loss(self, true_obj, true_class, pred_class):\n",
    "        \"\"\"\n",
    "        calculate loss of class prediction\n",
    "\n",
    "        inputs:\n",
    "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
    "        true_class: one-hot class from ground truth in shape of (batch, grid, grid, anchor, num_classes)\n",
    "        pred_class: one-hot class from model prediction in shape of (batch, grid, grid, anchor, num_classes)\n",
    "\n",
    "        outputs:\n",
    "        class_loss: class loss\n",
    "        \"\"\"\n",
    "        # Yolov1:\n",
    "        # \"Note that the loss function only penalizes classiﬁcation error\n",
    "        # if an object is present in that grid cell (hence the conditional\n",
    "        # class probability discussed earlier).\n",
    "        class_loss = binary_cross_entropy(pred_class, true_class)\n",
    "        class_loss = true_obj * class_loss\n",
    "        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3, 4))\n",
    "        return class_loss\n",
    "\n",
    "    def calc_xy_loss(self, true_obj, true_xy, pred_xy, weight):\n",
    "        \"\"\"\n",
    "        calculate loss of the centroid coordinate: sum of L2 distances\n",
    "\n",
    "        inputs:\n",
    "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
    "        true_xy: centroid x and y from ground truth in shape of (batch, grid, grid, anchor, 2)\n",
    "        pred_xy: centroid x and y from model prediction in shape of (batch, grid, grid, anchor, 2)\n",
    "        weight: weight adjustment, reward smaller bounding box\n",
    "\n",
    "        outputs:\n",
    "        xy_loss: centroid loss\n",
    "        \"\"\"\n",
    "        # shape (batch, grid, grid, anchor), eg. (32, 13, 13, 3)\n",
    "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n",
    "\n",
    "        # in order to element-wise multiply the result from tf.reduce_sum\n",
    "        # we need to squeeze one dimension for objectness here\n",
    "        true_obj = tf.squeeze(true_obj, axis=-1)\n",
    "\n",
    "        # YoloV1:\n",
    "        # \"It also only penalizes bounding box coordinate error if that\n",
    "        # predictor is \"responsible\" for the ground truth box (i.e. has the\n",
    "        # highest IOU of any predictor in that grid cell).\"\n",
    "        xy_loss = true_obj * xy_loss * weight\n",
    "\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3)) * self.lambda_coord\n",
    "\n",
    "        return xy_loss\n",
    "\n",
    "    def calc_wh_loss(self, true_obj, true_wh, pred_wh, weight):\n",
    "        \"\"\"\n",
    "        calculate loss of the width and height: sum of L2 distances\n",
    "\n",
    "        inputs:\n",
    "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
    "        true_wh: width and height from ground truth in shape of (batch, grid, grid, anchor, 2)\n",
    "        pred_wh: width and height from model prediction in shape of (batch, grid, grid, anchor, 2)\n",
    "        weight: weight adjustment, reward smaller bounding box\n",
    "\n",
    "        outputs:\n",
    "        wh_loss: width and height loss\n",
    "        \"\"\"\n",
    "        # shape (batch, grid, grid, anchor), eg. (32, 13, 13, 3)\n",
    "        wh_loss = tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n",
    "        true_obj = tf.squeeze(true_obj, axis=-1)\n",
    "        wh_loss = true_obj * wh_loss * weight\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3)) * self.lambda_coord\n",
    "        return wh_loss\n",
    "\n",
    "\n",
    "##  PREPROCESS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self, is_train, num_classes, output_shape=(416, 416)):\n",
    "        self.is_train = is_train\n",
    "        self.num_classes = num_classes\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "\n",
    "        encoded = features['image/encoded']\n",
    "        image = tf.io.decode_jpeg(encoded)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "        classes, bboxes = self.parse_y_features(features)\n",
    "        image, bboxes = self.random_flip_image_and_label(image, bboxes)\n",
    "        image, bboxes = self.random_crop_image_and_label(image, bboxes)\n",
    "\n",
    "        image = tf.image.resize(image, self.output_shape)\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "\n",
    "        label = (\n",
    "            self.preprocess_label_for_one_scale(classes, bboxes, 52,\n",
    "                                                np.array([0, 1, 2])),\n",
    "            self.preprocess_label_for_one_scale(classes, bboxes, 26,\n",
    "                                                np.array([3, 4, 5])),\n",
    "            self.preprocess_label_for_one_scale(classes, bboxes, 13,\n",
    "                                                np.array([6, 7, 8])),\n",
    "        )\n",
    "        return image, label\n",
    "\n",
    "    def random_flip_image_and_label(self, image, bboxes):\n",
    "        \"\"\"\n",
    "        flip left and right for 50% of images\n",
    "        \"\"\"\n",
    "        r = tf.random.uniform([1])\n",
    "        if r < 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "            xmin, ymin, xmax, ymax = tf.split(bboxes, [1, 1, 1, 1], -1)\n",
    "            # note that we need to switch here\n",
    "            xmin, xmax = 1 - xmax, 1 - xmin\n",
    "            bboxes = tf.squeeze(\n",
    "                tf.stack([xmin, ymin, xmax, ymax], axis=1), axis=-1)\n",
    "\n",
    "        return image, bboxes\n",
    "\n",
    "    def get_random_crop_delta(self, bboxes):\n",
    "        \"\"\"\n",
    "        get a random crop which includes all bounding boxes. Since all bboxes here belong to one image,\n",
    "        we can calcualte the minimum of all xmin and ymin, and the maximum of all xmax and ymax to get\n",
    "        the an area that can include all boxes. the crop will be randomly picked between this area boundary and\n",
    "        the boundary of the whole image.\n",
    "        \"\"\"\n",
    "        min_xmin = tf.math.reduce_min(bboxes[..., 0])\n",
    "        min_ymin = tf.math.reduce_min(bboxes[..., 1])\n",
    "        max_xmax = tf.math.reduce_max(bboxes[..., 2])\n",
    "        max_ymax = tf.math.reduce_max(bboxes[..., 3])\n",
    "\n",
    "        # delta is the normalized margin from bboxes boundary the crop boundary\n",
    "        # ____________________________________\n",
    "        # |         ________________         |\n",
    "        # |image    |crop ______   |         |\n",
    "        # |<-DELTA->|     |bbox|   |<-DELTA->|\n",
    "        # |         |     |____|   |         |\n",
    "        # |         |______________|         |\n",
    "        # |__________________________________|\n",
    "        xmin_delta = tf.random.uniform([1], 0, min_xmin)\n",
    "        ymin_delta = tf.random.uniform([1], 0, min_ymin)\n",
    "        xmax_delta = tf.random.uniform([1], 0, 1 - max_xmax)\n",
    "        ymax_delta = tf.random.uniform([1], 0, 1 - max_ymax)\n",
    "\n",
    "        return xmin_delta, ymin_delta, xmax_delta, ymax_delta\n",
    "\n",
    "    def random_crop_image_and_label(self, image, bboxes):\n",
    "        \"\"\"\n",
    "        crop images randomly at 50% chance but preserve all bounding boxes. the crop is guaranteed to include\n",
    "        all bounding boxes. \n",
    "        \"\"\"\n",
    "        r = tf.random.uniform([1])\n",
    "        if r < 0.5:\n",
    "            xmin_delta, ymin_delta, xmax_delta, ymax_delta = self.get_random_crop_delta(\n",
    "                bboxes)\n",
    "\n",
    "            xmin, ymin, xmax, ymax = tf.split(bboxes, [1, 1, 1, 1], -1)\n",
    "            # before crop: |_0.1_|_0.1_|____________0.5___________|_0.1_|___0.2___|\n",
    "            # after crop:  |_0.1_|____________0.5___________|_0.1_|\n",
    "            # imagine old xmin is 0.2 (0.1+0.1), old xmax is 0.8 (0.1+0.1+0.5+0.1)\n",
    "            # if we cut both left 0.1 (xmin_delta) and right 0.2 (xmax_delta)\n",
    "            # the new xmin will be (0.2 - 0.1) / (1 - 0.1 - 0.2) = 1/7\n",
    "            # the new xmax will be (0.8 - 0.1) / (1 - 0.1 - 0.2) = 6/7\n",
    "            # same thing for y\n",
    "            xmin = (xmin - xmin_delta) / (1 - xmin_delta - xmax_delta)\n",
    "            ymin = (ymin - ymin_delta) / (1 - ymin_delta - ymax_delta)\n",
    "            xmax = (xmax - xmin_delta) / (1 - xmin_delta - xmax_delta)\n",
    "            ymax = (ymax - ymin_delta) / (1 - ymin_delta - ymax_delta)\n",
    "\n",
    "            bboxes = tf.squeeze(\n",
    "                tf.stack([xmin, ymin, xmax, ymax], axis=1), axis=-1)\n",
    "            h = tf.cast(tf.shape(image)[0], dtype=tf.float32)\n",
    "            w = tf.cast(tf.shape(image)[1], dtype=tf.float32)\n",
    "\n",
    "            offset_height = tf.cast(ymin_delta[0] * h, dtype=tf.int32)\n",
    "            offset_width = tf.cast(xmin_delta[0] * w, dtype=tf.int32)\n",
    "            target_height = tf.cast(\n",
    "                tf.math.ceil((1 - ymax_delta - ymin_delta)[0] * h),\n",
    "                dtype=tf.int32)\n",
    "            target_width = tf.cast(\n",
    "                tf.math.ceil((1 - xmax_delta - xmin_delta)[0] * w),\n",
    "                dtype=tf.int32)\n",
    "\n",
    "            image = image[offset_height:offset_height +\n",
    "                          target_height, offset_width:offset_width +\n",
    "                          target_width, :]\n",
    "        return image, bboxes\n",
    "\n",
    "    def parse_y_features(self, features):\n",
    "        classes = tf.sparse.to_dense(features['image/object/class/label'])\n",
    "        classes = tf.one_hot(classes, self.num_classes)\n",
    "\n",
    "        # tf.pad(classes, [[0, 100 - tf.shape(classes)[0]], []], 'CONSTANT')\n",
    "\n",
    "        # bboxes shape (None, 4)\n",
    "        bboxes = tf.stack([\n",
    "            tf.sparse.to_dense(features['image/object/bbox/xmin']),\n",
    "            tf.sparse.to_dense(features['image/object/bbox/ymin']),\n",
    "            tf.sparse.to_dense(features['image/object/bbox/xmax']),\n",
    "            tf.sparse.to_dense(features['image/object/bbox/ymax']),\n",
    "        ],\n",
    "                          axis=1)\n",
    "        return classes, bboxes\n",
    "\n",
    "    def preprocess_label_for_one_scale(self,\n",
    "                                       classes,\n",
    "                                       bboxes,\n",
    "                                       grid_size=13,\n",
    "                                       valid_anchors=None):\n",
    "        \"\"\"\n",
    "        preprocess the class and bounding boxes annotations into model desired format for one scale\n",
    "        (grid, grid, anchor, (centroid x, centroid y, width, height, objectness, ...one-hot classes...))\n",
    "\n",
    "        inputs:\n",
    "        grid_size: a scalar grid size to use\n",
    "\n",
    "        outputs:\n",
    "        y: the desired label format to calcualte loss\n",
    "        \"\"\"\n",
    "        # construct an empty placeholder for the final output y first\n",
    "        y = tf.zeros((grid_size, grid_size, 3, 5 + self.num_classes))\n",
    "\n",
    "        # find the best anchor indices for each ground truth box\n",
    "        anchor_indices = self.find_best_anchor(bboxes)\n",
    "\n",
    "        # necessary assertion, otherwise the steps later would fail\n",
    "        tf.Assert(classes.shape[0] == bboxes.shape[0], [classes])\n",
    "        tf.Assert(anchor_indices.shape[0] == bboxes.shape[0], [anchor_indices])\n",
    "\n",
    "        # this has to be tf.shape instead of classes.shape, otherwise would be None\n",
    "        num_boxes = tf.shape(classes)[0]\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        valid_count = 0\n",
    "        for i in tf.range(num_boxes):\n",
    "            curr_class = tf.cast(classes[i], tf.float32)\n",
    "            curr_box = bboxes[i]\n",
    "            curr_anchor = anchor_indices[i]\n",
    "\n",
    "            # only use the anchor when it belongs to current scale (grid_size)\n",
    "            # for example, when grid size is 13, only anchor 6, 7, 8 (big anchors) are valid\n",
    "            # because the reception field of this grid size is the biggest\n",
    "            # however, if grid size is 52, the finest grained grid, we can only use anchor\n",
    "            # 0, 1, 2 (small anchors)\n",
    "            anchor_found = tf.reduce_any(curr_anchor == valid_anchors)\n",
    "            if anchor_found:\n",
    "                # now that we found the anchor, we need to set it in our final output y\n",
    "                # we only have three anchor boxes in y, so we need to mod by 3 first to get\n",
    "                # adjusted index. eg. anchor 7 will have index 1\n",
    "                # we need to reshape here so that adjusted_anchor_index is a vector\n",
    "                adjusted_anchor_index = tf.math.floormod(curr_anchor, 3)\n",
    "\n",
    "                # we need to turn (xmin, ymin, xmax, ymax) box format into\n",
    "                # (centeroid x, centroid y, width, height) to be able to\n",
    "                # calculate yolo loss later\n",
    "                curr_box_xy = (curr_box[..., 0:2] + curr_box[..., 2:4]) / 2\n",
    "                curr_box_wh = curr_box[..., 2:4] - curr_box[..., 0:2]\n",
    "\n",
    "                # calculate which grid cell should we use\n",
    "                # eg. when curr_box_xy = [0.25, 0.25], and grid size = 26, which is a quarter of the image\n",
    "                # the index of grid cell is floor(0.25 * 26) = 6\n",
    "                grid_cell_xy = tf.cast(\n",
    "                    curr_box_xy // tf.cast((1 / grid_size), dtype=tf.float32),\n",
    "                    tf.int32)\n",
    "\n",
    "                # for this box, we need to update y at location (grid_size, grid_size, adjusted_anchor_index)\n",
    "                # eg. shape in (13, 13, 1)\n",
    "                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n",
    "                # note that it's not grid[x][y]\n",
    "                index = tf.stack(\n",
    "                    [grid_cell_xy[1], grid_cell_xy[0], adjusted_anchor_index])\n",
    "\n",
    "                # this is the value we use to update the above location\n",
    "                # eg. shape in (7)\n",
    "                # note that we need to make this one-hot classes in order to use categorical crossentropy later\n",
    "                update = tf.concat(\n",
    "                    values=[\n",
    "                        curr_box_xy, curr_box_wh,\n",
    "                        tf.constant([1.0]), curr_class\n",
    "                    ],\n",
    "                    axis=0)\n",
    "                # add to final indices and updates to be written into y\n",
    "                indices = indices.write(valid_count, index)\n",
    "                updates = updates.write(valid_count, update)\n",
    "                # tf.print(indices.stack())\n",
    "                # tf.print(updates.stack())\n",
    "                valid_count = 1 + valid_count\n",
    "\n",
    "        y = tf.tensor_scatter_nd_update(y, indices.stack(), updates.stack())\n",
    "        return y\n",
    "\n",
    "    def find_best_anchor(self, y_box):\n",
    "        \"\"\"\n",
    "        find the best anchor for num_boxes ground truth boxes in y_box. Return a tensor in shape\n",
    "        of (num_boxes) that indicates the indices of best anchor for each box\n",
    "\n",
    "        inputs:\n",
    "        y_box: ground truth boxes in shape of (num_boxes, 4)\n",
    "\n",
    "        outputs:\n",
    "        anchor_idx: anchor indices in shape of (num_boxes)\n",
    "        \"\"\"\n",
    "        box_wh = y_box[..., 2:4] - y_box[..., 0:2]\n",
    "\n",
    "        box_wh = tf.tile(\n",
    "            tf.expand_dims(box_wh, -2), (1, tf.shape(anchors_wh)[0], 1))\n",
    "\n",
    "        \n",
    "        intersection = tf.minimum(box_wh[..., 0],\n",
    "                                  anchors_wh[..., 0]) * tf.minimum(\n",
    "                                      box_wh[..., 1], anchors_wh[..., 1])\n",
    "\n",
    "        # box_area is the width*height for each box\n",
    "        # eg box_area -> (2, 9)\n",
    "        box_area = box_wh[..., 0] * box_wh[..., 1]\n",
    "\n",
    "        # anchor area is the width*height for each anchor\n",
    "        # eg anchor_area -> (9)\n",
    "        anchor_area = anchors_wh[..., 0] * anchors_wh[..., 1]\n",
    "\n",
    "        # eg. iou -> (2, 9)\n",
    "        iou = intersection / (box_area + anchor_area - intersection)\n",
    "\n",
    "        # find the best anchor for each box, there should be num_boxes indices\n",
    "        # in the result\n",
    "        # eg. anchor_idx -> (2)\n",
    "        anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.int32)\n",
    "        return anchor_idx\n",
    "\n",
    "    def parse_tfexample(self, example_proto):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto,\n",
    "                                          image_feature_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb76021-3e35-4a58-92ca-8721b05ab769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TOTAL_CLASSES = 80\n",
    "TOTAL_EPOCHS = 300\n",
    "OUTPUT_SHAPE = (416, 416)\n",
    "TF_RECORDS = 'Datasets/yolo/tfrecords'\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 initial_epoch,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate=0.01):\n",
    "        self.model = model\n",
    "        self.initial_epoch = initial_epoch\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_objects = [\n",
    "            YoloLoss(\n",
    "                num_classes=TOTAL_CLASSES,\n",
    "                valid_anchors_wh=anchors_wh[0:3]),  # small scale 52x52\n",
    "            YoloLoss(\n",
    "                num_classes=TOTAL_CLASSES,\n",
    "                valid_anchors_wh=anchors_wh[3:6]),  # medium scale 26x26\n",
    "            YoloLoss(\n",
    "                num_classes=TOTAL_CLASSES,\n",
    "                valid_anchors_wh=anchors_wh[6:9]),  # large scale 13x13\n",
    "        ]\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "\n",
    "        # for learning rate schedule\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "\n",
    "    def lr_decay(self):\n",
    "        \"\"\"\n",
    "        This effectively simulate ReduceOnPlateau learning rate schedule. Learning rate\n",
    "        will be reduced by a factor of 10 if there's no improvement over [max_patience] epochs\n",
    "        \"\"\"\n",
    "        if self.patience_count > self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            total_losses = []\n",
    "            xy_losses = []\n",
    "            wh_losses = []\n",
    "            class_losses = []\n",
    "            obj_losses = []\n",
    "            # iterate over all three scales\n",
    "            for loss_object, y_pred, y_true in zip(self.loss_objects, outputs,\n",
    "                                                   labels):\n",
    "                total_loss, loss_breakdown = loss_object(y_true, y_pred)\n",
    "                xy_loss, wh_loss, class_loss, obj_loss = loss_breakdown\n",
    "                total_losses.append(total_loss * (1. / self.global_batch_size))\n",
    "                xy_losses.append(xy_loss * (1. / self.global_batch_size))\n",
    "                wh_losses.append(wh_loss * (1. / self.global_batch_size))\n",
    "                class_losses.append(class_loss * (1. / self.global_batch_size))\n",
    "                obj_losses.append(obj_loss * (1. / self.global_batch_size))\n",
    "\n",
    "            total_loss = tf.reduce_sum(total_losses)\n",
    "            total_xy_loss = tf.reduce_sum(xy_losses)\n",
    "            total_wh_loss = tf.reduce_sum(wh_losses)\n",
    "            total_class_loss = tf.reduce_sum(class_losses)\n",
    "            total_obj_loss = tf.reduce_sum(obj_losses)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=total_loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return total_loss, (total_xy_loss, total_wh_loss, total_class_loss,\n",
    "                            total_obj_loss)\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "\n",
    "        outputs = self.model(images, training=False)\n",
    "        losses = []\n",
    "        # iterate over all three scales\n",
    "        for loss_object, y_pred, y_true in zip(self.loss_objects, outputs,\n",
    "                                               labels):\n",
    "            loss, _ = loss_object(y_true, y_pred)\n",
    "            losses.append(loss * (1. / self.global_batch_size))\n",
    "        total_loss = tf.reduce_sum(losses)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def get_current_time(self):\n",
    "        return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        total_steps = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset, train_summary_writer,\n",
    "                                    total_steps):\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = tf.constant(0, dtype=tf.int64)\n",
    "            for one_batch in dataset:\n",
    "                per_replica_losses, per_replica_losses_breakdown = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                per_replica_xy_losses, per_replica_wh_losses, per_replica_class_losses, per_replica_obj_losses = per_replica_losses_breakdown\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "                batch_xy_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM,\n",
    "                    per_replica_xy_losses,\n",
    "                    axis=None)\n",
    "                batch_wh_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM,\n",
    "                    per_replica_wh_losses,\n",
    "                    axis=None)\n",
    "                batch_class_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM,\n",
    "                    per_replica_class_losses,\n",
    "                    axis=None)\n",
    "                batch_obj_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM,\n",
    "                    per_replica_obj_losses,\n",
    "                    axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch:', num_train_batches, 'batch loss:',\n",
    "                         batch_loss, 'batch xy loss', batch_xy_loss,\n",
    "                         'batch wh loss', batch_wh_loss, 'batch obj loss',\n",
    "                         batch_obj_loss, 'batch_class_loss', batch_class_loss,\n",
    "                         'epoch total loss:', total_loss)\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        'batch train loss',\n",
    "                        batch_loss,\n",
    "                        step=total_steps + num_train_batches)\n",
    "                    tf.summary.scalar(\n",
    "                        'batch xy loss',\n",
    "                        batch_xy_loss,\n",
    "                        step=total_steps + num_train_batches)\n",
    "                    tf.summary.scalar(\n",
    "                        'batch wh loss',\n",
    "                        batch_wh_loss,\n",
    "                        step=total_steps + num_train_batches)\n",
    "                    tf.summary.scalar(\n",
    "                        'batch obj loss',\n",
    "                        batch_obj_loss,\n",
    "                        step=total_steps + num_train_batches)\n",
    "                    tf.summary.scalar(\n",
    "                        'batch class loss',\n",
    "                        batch_class_loss,\n",
    "                        step=total_steps + num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = tf.constant(0, dtype=tf.int64)\n",
    "            for one_batch in dataset:\n",
    "                per_replica_losses = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_val_batches += 1\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        current_time = self.get_current_time()\n",
    "        train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "        val_log_dir = 'logs/gradient_tape/' + current_time + '/val'\n",
    "        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "        val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "        tf.print('{} Start training...'.format(current_time))\n",
    "        for epoch in range(self.initial_epoch, self.epochs + 1):\n",
    "            t0 = time.time()\n",
    "            self.lr_decay()\n",
    "\n",
    "            tf.print(\n",
    "                '{} Started epoch {} with learning rate {}. Current LR patience count is {} epochs. Last lowest val loss is {}.'\n",
    "                .format(self.get_current_time(), epoch,\n",
    "                        self.current_learning_rate, self.patience_count,\n",
    "                        self.lowest_val_loss))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset, train_summary_writer, total_steps)\n",
    "            t1 = time.time()\n",
    "            train_loss = train_total_loss / tf.cast(\n",
    "                num_train_batches, dtype=tf.float32)\n",
    "            tf.print(\n",
    "                '{} Epoch {} train loss {}, total train batches {}, {} examples per second'\n",
    "                .format(\n",
    "                    self.get_current_time(), epoch, train_loss,\n",
    "                    num_train_batches,\n",
    "                    tf.cast(num_train_batches, dtype=tf.float32) *\n",
    "                    self.global_batch_size / (t1 - t0)))\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('epoch train loss', train_loss, step=epoch)\n",
    "            total_steps += num_train_batches\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "\n",
    "            t2 = time.time()\n",
    "            val_loss = val_total_loss / tf.cast(\n",
    "                num_val_batches, dtype=tf.float32)\n",
    "            tf.print(\n",
    "                '{} Epoch {} val loss {}, total val batches {}, {} examples per second'\n",
    "                .format(\n",
    "                    self.get_current_time(), epoch, val_loss, num_val_batches,\n",
    "                    tf.cast(num_val_batches, dtype=tf.float32) *\n",
    "                    self.global_batch_size / (t2 - t1)))\n",
    "            with val_summary_writer.as_default():\n",
    "                tf.summary.scalar('epoch val loss', val_loss, step=epoch)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        self.save_model(self.epochs, self.last_val_loss)\n",
    "        print('{} Finished.'.format(self.get_current_time()))\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        # https://github.com/tensorflow/tensorflow/issues/33565\n",
    "        model_name = 'Datasets/yolo/models/model-v1.0.1-epoch-{}-loss-{:.4f}.tf'.format(\n",
    "            epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61cd3c04-67f2-4dba-a3c6-3a3973cd6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: Datasets/yolo/tfrecords/train*'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mrun(train_dist_dataset, val_dist_dataset)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mMirroredStrategy()\n\u001b[1;32m     23\u001b[0m global_batch_size \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync \u001b[38;5;241m*\u001b[39m BATCH_SIZE\n\u001b[0;32m---> 24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/train*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTF_RECORDS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m create_dataset(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/val*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(TF_RECORDS), global_batch_size, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatasets/yolo/models/\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(tfrecords, batch_size, is_train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(tfrecords, batch_size, is_train):\n\u001b[1;32m      2\u001b[0m     preprocess \u001b[38;5;241m=\u001b[39m Preprocessor(is_train, TOTAL_CLASSES, OUTPUT_SHAPE)\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfrecords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(dataset)\n\u001b[1;32m      6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      7\u001b[0m         preprocess, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:1303\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[1;32m   1296\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1297\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1299\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1301\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[1;32m   1306\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[1;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: Datasets/yolo/tfrecords/train*'"
     ]
    }
   ],
   "source": [
    "def create_dataset(tfrecords, batch_size, is_train):\n",
    "    preprocess = Preprocessor(is_train, TOTAL_CLASSES, OUTPUT_SHAPE)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(512)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--checkpoint', type=str, help='checkpoint file path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * BATCH_SIZE\n",
    "    train_dataset = create_dataset(\n",
    "        '{}/train*'.format(TF_RECORDS), global_batch_size, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        '{}/val*'.format(TF_RECORDS), global_batch_size, is_train=False)\n",
    "    if not os.path.exists(os.path.join('Datasets/yolo/models/')):\n",
    "        os.makedirs(os.path.join('Datasets/yolo/models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "        model = YoloV3(\n",
    "            shape=(416, 416, 3), num_classes=TOTAL_CLASSES, training=True)\n",
    "        model.summary()\n",
    "        model.load_weights(checkpoint)\n",
    "        initial_epoch = int(args.checkpoint.split('-')[-3]) + 1\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            initial_epoch=initial_epoch,\n",
    "            epochs=TOTAL_EPOCHS,\n",
    "            global_batch_size=global_batch_size,\n",
    "            strategy=strategy,\n",
    "        )\n",
    "        trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4d4c00-2682-445f-ba3d-e19c181ef544",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: Datasets/yolo/tfrecords/val_0004_of_0008.tfrecords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m Preprocessor(\u001b[38;5;28;01mFalse\u001b[39;00m, TOTAL_CLASSES, (\u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatasets/yolo/tfrecords/val_0004_of_0008.tfrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(dataset)\n\u001b[1;32m     16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(preprocess, num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:1303\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[1;32m   1296\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1297\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1299\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1301\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[1;32m   1306\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn38/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[1;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: Datasets/yolo/tfrecords/val_0004_of_0008.tfrecords'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "TOTAL_CLASSES = 80\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "preprocess = Preprocessor(False, TOTAL_CLASSES, (416, 416))\n",
    "dataset = tf.data.Dataset.list_files('Datasets/yolo/tfrecords/val_0004_of_0008.tfrecords')\n",
    "dataset = tf.data.TFRecordDataset(dataset)\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=1)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "x, y_true = next(iter(dataset))\n",
    "\n",
    "\n",
    "class_names = {}\n",
    "with open('Datasets/yolo/MSCOCO/mscoco_2017_names.txt') as fp:\n",
    "    lines = fp.read().splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        class_names[i] = line\n",
    "\n",
    "\n",
    "model = YoloV3(shape=(416, 416, 3), num_classes=TOTAL_CLASSES, training=False)\n",
    "model.load_weights('Datasets/yolo/models/model-v1.0.1-epoch-56-loss-42.0143.tf')\n",
    "\n",
    "\n",
    "postprocess = Postprocessor(iou_thresh=0.5, score_thresh=0.5)\n",
    "y_pred = model(x, training=False)\n",
    "y_pred_nms = postprocess(y_pred)\n",
    "\n",
    "num_predictions = y_pred_nms[3][0][0].numpy()\n",
    "print('Number of predictions: ', num_predictions)\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "im = tf.cast((x[0] + 1) * 127.5, tf.int32)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "h, w, d = im.shape\n",
    "\n",
    "for i in range(0, num_predictions):\n",
    "    box = y_pred_nms[0][0][i].numpy()\n",
    "    score = y_pred_nms[1][0][i][0].numpy()\n",
    "    class_prob = y_pred_nms[2][0][i].numpy()\n",
    "    values, indices = tf.math.top_k(class_prob, k=3)\n",
    "    text= []\n",
    "    for index, prob in zip(indices.numpy(), values.numpy()):\n",
    "        text.append('{} {:.1f}%'.format(class_names[index], prob*100))\n",
    "    text = ', '.join(text)\n",
    "    xmin = box[0] * w\n",
    "    ymin = box[1] * h\n",
    "    width = (box[2] - box[0]) * w\n",
    "    height = (box[3] - box[1]) * h\n",
    "    color = colors[i % 8]\n",
    "    rect = patches.Rectangle((xmin,ymin),width,height,linewidth=2,edgecolor=color,facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.annotate(text, (xmin, ymin - 5), color=color, weight='bold', \n",
    "                fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc80f20-d83f-423e-a2fb-5307c250aa53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
