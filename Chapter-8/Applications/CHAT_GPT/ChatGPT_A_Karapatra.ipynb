{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2afbce-7f9f-40b1-9e3e-c9b0f4f41d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9e2466e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "Copyright (c) 2023, 2024 , Prof. Radhamadhab Dalai, ITER , Siksha O Aanusandhan University, \n",
    "Odisha, India\n",
    "Author's email address :  radhamadhabdalai@soa.ac.in\n",
    "###################################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd05789d-eb69-4337-b93b-37d8a107e847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset/shakespeare.txt','r',encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "\n",
    "class CharacterLevelTokenizer:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.vocab = sorted(list(set(self.data)))\n",
    "        self.VOCAB_SIZE = len(self.vocab)\n",
    "        \n",
    "        self.i_s = {i:s for i,s in enumerate(self.vocab)}\n",
    "        self.s_i = {s:i for i,s in self.i_s.items()}\n",
    "        \n",
    "    def encode(self,s):\n",
    "        return torch.tensor([self.s_i[c] for c in s],dtype=torch.long)\n",
    "\n",
    "    def decode(self,s):\n",
    "        return ''.join([self.i_s[i.item()] for i in s])\n",
    "\n",
    "tokenizer = CharacterLevelTokenizer(data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    block_size = 256 # context-length\n",
    "    batch_size = 64 # mini-batch size\n",
    "    vocab_size = tokenizer.VOCAB_SIZE\n",
    "    n_embed = 256\n",
    "    n_heads = 8\n",
    "    head_size = n_embed // n_heads # computes to 384/6=64 or 128/4=32 or 256/8\n",
    "    \n",
    "    n_layers = 3\n",
    "    \n",
    "    train_iters = 10_000\n",
    "    val_iters = 1000\n",
    "    lr = 3e-4\n",
    "    \n",
    "    attn_dropout = 0.1\n",
    "    block_dropout = 0.1\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "Config.device\n",
    "# Single Attention Head\n",
    "class ShakespeareDataset:\n",
    "    def __init__(self,Config, is_test=False) -> None:\n",
    "        self.tokenizer = CharacterLevelTokenizer(data)\n",
    "        self.is_test = is_test\n",
    "        self.full_data = self.tokenizer.encode(self.tokenizer.data)\n",
    "        if self.is_test:\n",
    "            self.data = self.full_data[int(0.9*len(self.full_data)):]\n",
    "        else:\n",
    "            self.data = self.full_data[:int(0.9*len(self.full_data))]\n",
    "        self.block_size = Config.block_size\n",
    "        self.batch_size = Config.batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_block_size(self) -> int:\n",
    "        return self.block_size\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.tokenizer.VOCAB_SIZE\n",
    "\n",
    "    def get(self):\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([self.data[i:i+self.block_size] for i in ix])\n",
    "        y = torch.stack([self.data[i+1:i+self.block_size+1] for i in ix])\n",
    "        return x,y\n",
    "\n",
    "#GPT model\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.n_embed = Config.n_embed\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        \n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'tril',\n",
    "            torch.tril(torch.ones(self.block_size,self.block_size))\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q@k.transpose(-2,-1) * (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out\n",
    "\n",
    "#Multihead attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = Config.n_heads\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(Config) for _ in range(self.n_heads)])\n",
    "        \n",
    "        self.projection = nn.Linear(Config.n_embed, Config.n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "#Feedforward layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(Config.n_embed,Config.n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Config.n_embed * 4, Config.n_embed), # projection\n",
    "            nn.Dropout(Config.block_dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "# transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(Config)\n",
    "        self.ff = FeedForward(Config)\n",
    "        self.ln1 = nn.LayerNorm(Config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(Config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d9bb2-c06f-4a52-a965-215693062c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT model\n",
    "\n",
    "class ShakespeareGPT(nn.Module):\n",
    "    def __init__(self,Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embed = Config.n_embed\n",
    "        self.block_size = Config.block_size\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(Config.vocab_size,self.n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(self.block_size, self.n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(Config)]*Config.n_layers,\n",
    "            nn.LayerNorm(self.n_embed)\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(self.n_embed,Config.vocab_size)\n",
    "        \n",
    "    def forward(self,idx):\n",
    "        \n",
    "        B,T = idx.shape\n",
    "        \n",
    "        token_embs = self.token_embedding_table(idx)\n",
    "        pos_embs = self.pos_embedding_table(torch.arange(T,device=Config.device))\n",
    "        \n",
    "        \n",
    "        x = token_embs + pos_embs\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "        \n",
    "    def generate(self,idx,total):\n",
    "        for _ in range(total):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits= self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "#training\n",
    "\n",
    "train_ds = ShakespeareDataset(Config)\n",
    "val_ds = ShakespeareDataset(Config,is_test=True)\n",
    "\n",
    "lm = ShakespeareGPT(Config)\n",
    "lm = lm.to(device=Config.device)\n",
    "\n",
    "optim = torch.optim.AdamW(lm.parameters(), lr=Config.lr)\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    B,T,C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits,targets)\n",
    "    return loss\n",
    "\n",
    "def train_N_iters():\n",
    "    lm.train()\n",
    "    train_step_losses = []\n",
    "    for batch in tqdm(range(Config.train_iters)):\n",
    "        optim.zero_grad()\n",
    "        inputs, targets = train_ds.get()\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_step_losses.append(loss.item())\n",
    "        \n",
    "        if batch%(Config.train_iters//10)==0 or batch==Config.train_iters-1:\n",
    "            print(f\"batch {batch} train step loss: {loss.item()}\")\n",
    "        \n",
    "        del inputs, targets, loss, logits\n",
    "        \n",
    "    return train_step_losses\n",
    "    \n",
    "@torch.no_grad()\n",
    "def valid_N_iters():\n",
    "    lm.eval()\n",
    "    val_step_losses = []\n",
    "    for batch in tqdm(range(Config.val_iters)):\n",
    "        inputs, targets = val_ds.get()\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        val_step_losses.append(loss.item())\n",
    "        \n",
    "        if batch%(Config.val_iters//10)==0 or batch==Config.val_iters-1:\n",
    "            print(f\"batch {batch} valid step loss: {loss.item()}\")\n",
    "        \n",
    "        del inputs, targets, loss, logits\n",
    "    \n",
    "    return val_step_losses\n",
    "\n",
    "\n",
    "def save_lm():\n",
    "    state_dict = lm.state_dict()\n",
    "    save_path = Path('./').resolve() / 'shakespeareGPT'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    model_path = save_path / f'shakespeareGPT.pth'\n",
    "    torch.save(state_dict, model_path)\n",
    "\n",
    "def train_lm():\n",
    "    train_losses = train_N_iters()\n",
    "    valid_losses = valid_N_iters()\n",
    "    save_lm()\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "tl,vl=train_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae96a43-2bb0-4507-9d48-6919665b7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tl,label='train loss',color='orange')\n",
    "plt.plot(vl,label='valid loss',color='blue')\n",
    "plt.title('Shakespeare GPT Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "generated_texts = []\n",
    "for length in [100,300,500,700,1000]:\n",
    "    generated = lm.generate(\n",
    "    torch.zeros((1,1),dtype=torch.long,device=Config.device), # initial context 0\n",
    "    total=length\n",
    ")\n",
    "    generated = tokenizer.decode(generated[0])\n",
    "    text=f'generated ({length} tokens)\\n{\"=\"*50}\\n{generated}\\n{\"=\"*50}\\n\\n'\n",
    "    generated_texts.append(text)\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
